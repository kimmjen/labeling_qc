#!/usr/bin/env python3
"""
CLI ë„êµ¬ - í’ˆì§ˆ ê²€ìˆ˜ ëª…ë ¹í–‰ ì¸í„°í˜ì´ìŠ¤
"""

import argparse
import sys
import time
import json
import re
import os
from pathlib import Path
import PyPDF2
from tqdm import tqdm
import shutil
import zipfile

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ sys.pathì— ì¶”ê°€
sys.path.insert(0, str(Path(__file__).parent.parent))

from src.core.quality_controller import QualityController
from src.utils.quality_comparator import QualityComparator
from src.utils.zip_processor import ZipProcessor
from src.utils.zip_recompressor import ZipRecompressor
from src.core.rule_fixer import RuleBasedFixer
from src.services.pdf_uploader import PDFUploader


def safe_rmtree(path, max_attempts=3, delay=0.5):
    """
    ì•ˆì „í•œ í´ë” ì‚­ì œ í•¨ìˆ˜ - ê¶Œí•œ ì˜¤ë¥˜ ì‹œ ì¬ì‹œë„
    """
    from pathlib import Path
    import time
    
    path = Path(path)
    if not path.exists():
        return True
        
    for attempt in range(max_attempts):
        try:
            shutil.rmtree(path)
            return True
        except PermissionError:
            if attempt < max_attempts - 1:
                time.sleep(delay)
            else:
                print(f"âš ï¸ í´ë” ì‚­ì œ ì‹¤íŒ¨ (ê¶Œí•œ ì˜¤ë¥˜): {path}")
                return False
        except Exception as e:
            print(f"âš ï¸ í´ë” ì‚­ì œ ì¤‘ ì˜¤ë¥˜: {e}")
            return False
    return False


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(
        description="""
ğŸ“‹ ë¼ë²¨ë§ í’ˆì§ˆ ê²€ìˆ˜ ë„êµ¬ v2.0

ì´ ë„êµ¬ëŠ” ë¼ë²¨ë§ ë°ì´í„°ì˜ í’ˆì§ˆì„ ê²€ì¦í•˜ê³  ìë™ìœ¼ë¡œ ìˆ˜ì •í•˜ëŠ” CLI ë„êµ¬ì…ë‹ˆë‹¤.
JSON íŒŒì¼ ë‚´ì˜ ë¼ë²¨ë§ ì˜¤ë¥˜ë¥¼ ê°ì§€í•˜ê³ , í•œê¸€ ì¸ì½”ë”© ë¬¸ì œê¹Œì§€ í¬í•¨í•˜ì—¬ ì¢…í•©ì ì¸ í’ˆì§ˆ ê´€ë¦¬ë¥¼ ì œê³µí•©ë‹ˆë‹¤.

ğŸ” ì§€ì›ë˜ëŠ” ê²€ì¦ ê·œì¹™:
  R001: ì›ë¬¸/ë²ˆì—­ë¬¸ì´ ListTextì¸ ê²½ìš° â†’ ParaTextë¡œ ë³€ê²½
  R002: ìˆ«ìë‚˜ â–¡ê°€ ì—†ëŠ” RegionTitle â†’ ParaTitleë¡œ ë³€ê²½  
  R003: ë²•ë ¹ êµ¬ì¡° ë¼ë²¨ë§ (ì œ~í¸/ì¥/ì ˆ/ì¡°, ~ë²• â†’ ParaTitle)
  R004: í•œê¸€ ì¸ì½”ë”© ì˜¤ë¥˜ ê²€ì¶œ (ê¹¨ì§„ í…ìŠ¤íŠ¸, ì˜ëª»ëœ ì˜ì–´ ì¸ì‹)

ğŸ’¡ ì‚¬ìš© ì˜ˆì‹œ:
  # ë‹¨ìˆœ ê²€ì¦ë§Œ ì‹¤í–‰
  python cli/cli_tool.py "data/" --validate
  
  # ìë™ ìˆ˜ì • ì‹¤í–‰
  python cli/cli_tool.py "data/" --auto-fix
  
  # ê²€ìˆ˜ í´ë” ìƒì„±í•˜ì—¬ ì „ì²´ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰
  python cli/cli_tool.py "ì›ë³¸í´ë”/" --process-to-review
  
  # ëª¨ë“  ë¼ë²¨ì„ ListTextë¡œ í†µì¼ í›„ R003 ë²•ë ¹ êµ¬ì¡°ë§Œ ì ìš©
  python cli/cli_tool.py "zipí´ë”/" --listtext-only
  
  # ë‘ í´ë” ë¹„êµ ë¶„ì„
  python cli/cli_tool.py "ì›ë³¸/" --compare "ê²€ìˆ˜ì™„ë£Œ/"
  
  # ì „ì²´ ì›Œí¬í”Œë¡œìš° (ì¶”ì¶œâ†’ìˆ˜ì •â†’ì¬ì••ì¶•)
  python cli/cli_tool.py "data/" --full-workflow
        """,
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ğŸ“š ìƒì„¸ ì˜µì…˜ ì„¤ëª…:

ğŸ’¼ ê¸°ë³¸ ì‘ì—… ëª¨ë“œ:
  --validate (-v)     : í’ˆì§ˆ ê²€ì¦ë§Œ ì‹¤í–‰í•˜ê³  ê²°ê³¼ ì¶œë ¥
  --auto-fix          : ê²€ì¦ í›„ ìë™ ìˆ˜ì •ê¹Œì§€ ì‹¤í–‰
  --process-to-review : ê²€ìˆ˜ í´ë” ìƒì„±í•˜ì—¬ ìˆ˜ì •ëœ íŒŒì¼ ì €ì¥ (ê¶Œì¥)

ğŸ”§ ê³ ê¸‰ ì˜µì…˜:
  --full-workflow     : ZIP ì¶”ì¶œ â†’ ìˆ˜ì • â†’ ì¬ì••ì¶• ì „ì²´ ê³¼ì •
  --recompress        : ìˆ˜ì • í›„ ZIP íŒŒì¼ë¡œ ì¬ì••ì¶•
  --compare (-c)      : ë‘ í´ë”ì˜ ê²€ìˆ˜ ê²°ê³¼ ë¹„êµ

ğŸ“Š ì¶œë ¥ ì˜µì…˜:
  --report (-r)       : ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥
  --verbose           : ìƒì„¸í•œ ì²˜ë¦¬ ê³¼ì • ì¶œë ¥

ğŸš€ ê¶Œì¥ ì‚¬ìš©ë²•:
  1. í’ˆì§ˆ í™•ì¸: python cli/cli_tool.py "í´ë”ê²½ë¡œ/" --validate
  2. ìë™ ìˆ˜ì •: python cli/cli_tool.py "í´ë”ê²½ë¡œ/" --process-to-review
  3. ê²°ê³¼ ë¹„êµ: python cli/cli_tool.py "ì›ë³¸/" --compare "ìˆ˜ì •ë³¸/"

âš ï¸  ì£¼ì˜ì‚¬í•­:
  - ì›ë³¸ íŒŒì¼ ë°±ì—…ì„ ê¶Œì¥í•©ë‹ˆë‹¤
  - --process-to-review ì˜µì…˜ì´ ê°€ì¥ ì•ˆì „í•œ ë°©ë²•ì…ë‹ˆë‹¤
  - R004 ê·œì¹™ì€ í•œê¸€ ê¹¨ì§ì„ ê°ì§€í•˜ì§€ë§Œ ìë™ ìˆ˜ì •í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤
        """
    )
    
    # í•„ìˆ˜ ì¸ìˆ˜
    parser.add_argument(
        "path", 
        help="ê²€ìˆ˜í•  íŒŒì¼ ë˜ëŠ” ë””ë ‰í† ë¦¬ ê²½ë¡œ (ZIP íŒŒì¼ í¬í•¨ í´ë” ë˜ëŠ” JSON íŒŒì¼)"
    )
    
    # ì‘ì—… ëª¨ë“œ ì˜µì…˜
    mode_group = parser.add_argument_group('ì‘ì—… ëª¨ë“œ', 'ê²€ìˆ˜ ì‘ì—…ì˜ ì¢…ë¥˜ë¥¼ ì„ íƒí•˜ì„¸ìš”')
    mode_group.add_argument(
        "--validate", "-v", 
        action="store_true", 
        help="í’ˆì§ˆ ê²€ì¦ë§Œ ì‹¤í–‰ (ìˆ˜ì •í•˜ì§€ ì•ŠìŒ)"
    )
    mode_group.add_argument(
        "--auto-fix", 
        action="store_true", 
        help="ìë™ ìˆ˜ì • ì‹¤í–‰ (ì›ë³¸ íŒŒì¼ ì§ì ‘ ìˆ˜ì •)"
    )
    mode_group.add_argument(
        "--process-to-review", 
        action="store_true", 
        help="ê²€ìˆ˜ í´ë” ìƒì„±í•˜ì—¬ ìˆ˜ì •ëœ íŒŒì¼ ì €ì¥ (ê¶Œì¥ ë°©ë²•)"
    )
    mode_group.add_argument(
        "--full-workflow", 
        action="store_true", 
        help="ì „ì²´ ì›Œí¬í”Œë¡œìš°: ZIP ì¶”ì¶œ â†’ í’ˆì§ˆê²€ìˆ˜ â†’ ìë™ìˆ˜ì • â†’ ì¬ì••ì¶•"
    )
    mode_group.add_argument(
        "--listtext-only", 
        action="store_true", 
        help="ëª¨ë“  ë¼ë²¨(ParaText, RegionTitle ë“±)ì„ ListTextë¡œ í†µì¼ í›„ R003 ë²•ë ¹ êµ¬ì¡° ê·œì¹™ë§Œ ì ìš©"
    )

    mode_group.add_argument(
        "--listtext-only2", 
        action="store_true", 
        help="ëª¨ë“  ë¼ë²¨ì„ ListTextë¡œ í†µì¼ í›„ R003 ë²•ë ¹ êµ¬ì¡° ê·œì¹™ë§Œ ì ìš© (ë‹¨ì¼ ZIP íŒŒì¼ ë˜ëŠ” í´ë” ì§€ì›, í´ë” êµ¬ì¡° ìœ ì§€)"
    )

    mode_group.add_argument(
        "--paratext-only", 
        action="store_true", 
        help="ëª¨ë“  ë¼ë²¨(ParaText, RegionTitle ë“±)ì„ ListTextë¡œ í†µì¼ í›„ R003 ë²•ë ¹ êµ¬ì¡° ê·œì¹™ë§Œ ì ìš©"
    )
    mode_group.add_argument(
        "--count-pages", 
        action="store_true", 
        help="ZIP íŒŒì¼ë“¤ì˜ ì´ í˜ì´ì§€ ìˆ˜ë¥¼ ë¶„ì„í•˜ê³  í†µê³„ ì •ë³´ ì¶œë ¥"
    )
    mode_group.add_argument(
        "--count-pages-by-worker", 
        action="store_true", 
        help="ì‘ì—…ìë³„ í´ë”ì˜ í˜ì´ì§€ ìˆ˜ë¥¼ ë¶„ì„í•˜ê³  í†µí•© ë³´ê³ ì„œ ìƒì„±"
    )
    mode_group.add_argument(
        "--upload", 
        action="store_true", 
        help="PDF íŒŒì¼ë“¤ì„ APIë¡œ ì—…ë¡œë“œí•˜ì—¬ OCR ì²˜ë¦¬ í›„ visualcontent ZIP íŒŒì¼ë¡œ ì €ì¥"
    )
    
    # ë¹„êµ ë° ë¶„ì„ ì˜µì…˜
    analysis_group = parser.add_argument_group('ë¹„êµ ë° ë¶„ì„', 'ê²€ìˆ˜ ê²°ê³¼ë¥¼ ë¹„êµí•˜ê³  ë¶„ì„í•©ë‹ˆë‹¤')
    analysis_group.add_argument(
        "--compare", "-c", 
        metavar="ì™„ë£Œí´ë”ê²½ë¡œ",
        help="ê²€ìˆ˜ ì™„ë£Œ í´ë”ì™€ ë¹„êµí•˜ì—¬ ì •í™•ë„ ë¶„ì„"
    )
    
    # ì¶œë ¥ ë° ë³´ê³ ì„œ ì˜µì…˜
    output_group = parser.add_argument_group('ì¶œë ¥ ë° ë³´ê³ ì„œ', 'ê²°ê³¼ ì €ì¥ ë° ì¶œë ¥ ë°©ì‹ì„ ì„¤ì •í•©ë‹ˆë‹¤')
    output_group.add_argument(
        "--report", "-r", 
        metavar="ë³´ê³ ì„œíŒŒì¼.json",
        help="ê²€ìˆ˜ ê²°ê³¼ë¥¼ JSON ë³´ê³ ì„œë¡œ ì €ì¥í•  ê²½ë¡œ"
    )
    output_group.add_argument(
        "--verbose", 
        action="store_true", 
        help="ìƒì„¸í•œ ì²˜ë¦¬ ê³¼ì •ê³¼ ë””ë²„ê·¸ ì •ë³´ ì¶œë ¥"
    )
    
    # ê³ ê¸‰ ì˜µì…˜ (í•˜ìœ„ í˜¸í™˜ì„±)
    advanced_group = parser.add_argument_group('ê³ ê¸‰ ì˜µì…˜', 'íŠ¹ìˆ˜í•œ ìš©ë„ë¡œ ì‚¬ìš©ë˜ëŠ” ì˜µì…˜ë“¤')
    advanced_group.add_argument(
        "--fix", "-f", 
        action="store_true", 
        help="ìë™ ìˆ˜ì • ì‹¤í–‰ (--auto-fixì™€ ë™ì¼, í•˜ìœ„ í˜¸í™˜ì„±ìš©)"
    )
    advanced_group.add_argument(
        "--recompress", 
        action="store_true", 
        help="ìˆ˜ì • í›„ ZIP íŒŒì¼ë¡œ ì¬ì••ì¶•"
    )
    
    args = parser.parse_args()
    
    # ê²½ë¡œ ê²€ì¦
    target_path = Path(args.path)
    if not target_path.exists():
        print(f"âŒ ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {target_path}")
        sys.exit(1)
    
    # ì»¨íŠ¸ë¡¤ëŸ¬ ì´ˆê¸°í™”
    controller = QualityController()
    
    # ë¹„êµ ëª¨ë“œ
    if args.compare:
        comparator = QualityComparator()
        completed_dir = Path(args.compare)
        
        if not completed_dir.exists():
            print(f"âŒ ê²€ìˆ˜ì™„ë£Œ í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {completed_dir}")
            sys.exit(1)
        
        print(f"ğŸ” ê²€ìˆ˜ ë¹„êµ ì‹œì‘")
        print(f"ğŸ“‚ ê²€ìˆ˜ ëŒ€ìƒ: {target_path}")
        print(f"ğŸ“‚ ê²€ìˆ˜ ì™„ë£Œ: {completed_dir}")
        
        results = comparator.compare_directories(target_path, completed_dir)
        report = comparator.generate_comparison_report(results)
        comparator.print_summary(report)
        
        if args.report:
            with open(args.report, 'w', encoding='utf-8') as f:
                json.dump(report, f, ensure_ascii=False, indent=2)
            print(f"ğŸ“„ ë¹„êµ ë³´ê³ ì„œ ì €ì¥: {args.report}")
        
        return
    
    # ì‘ì—…ìë³„ í˜ì´ì§€ ìˆ˜ ê³„ì‚° ëª¨ë“œ
    if args.count_pages_by_worker:
        print("ğŸ“Š ì‘ì—…ìë³„ í˜ì´ì§€ ìˆ˜ ë¶„ì„ ì‹œì‘")
        print(f"ğŸ¯ ëŒ€ìƒ í´ë”: {target_path}")
        
        # í´ë” ë‚´ìš© ë¨¼ì € í™•ì¸
        print(f"ğŸ“‚ í´ë” ë‚´ìš© ìŠ¤ìº”...")
        for item in target_path.iterdir():
            if item.is_dir():
                zip_count = len(list(item.glob("*.zip")))
                print(f"   ğŸ“ {item.name}/ (ZIP: {zip_count}ê°œ)")
            elif item.is_file():
                print(f"   ğŸ“„ {item.name} ({item.suffix})")
        
        # ì‘ì—…ìë³„ í´ë” ìŠ¤ìº”
        worker_folders = []
        for item in target_path.iterdir():
            if item.is_dir() and not item.name.startswith('.') and item.name not in ['temp_page_count', 'temp_extract']:
                # í´ë” ë‚´ ZIP íŒŒì¼ í™•ì¸
                zip_files = list(item.glob("*.zip"))
                if zip_files:
                    worker_folders.append(item)
                    print(f"   âœ… ì‘ì—…ì í´ë” ì¸ì‹: {item.name}")
            elif item.is_file() and item.suffix.lower() == '.zip':
                # ì‘ì—…ìë³„ í†µí•© ZIP íŒŒì¼ì¸ ê²½ìš° (íŒŒì¼ëª…ì´ ì‘ì—…ìëª…)
                worker_folders.append(item)
                print(f"   âœ… ì‘ì—…ì ZIP íŒŒì¼ ì¸ì‹: {item.name}")
        
        if not worker_folders:
            print("âŒ ì‘ì—…ì í´ë” ë˜ëŠ” ì‘ì—…ìë³„ ZIP íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            print("ğŸ’¡ í™•ì¸ ì‚¬í•­:")
            print("   - í´ë” ë‚´ì— ZIP íŒŒì¼ì´ ìˆëŠ” í•˜ìœ„ í´ë”ê°€ ìˆëŠ”ì§€")
            print("   - ì‘ì—…ìëª….zip í˜•íƒœì˜ íŒŒì¼ì´ ìˆëŠ”ì§€")
            sys.exit(1)
        
        print(f"ğŸ‘¥ ë°œê²¬ëœ ì‘ì—…ì: {len(worker_folders)}ëª…")
        for worker_item in worker_folders:
            if worker_item.is_dir():
                zip_count = len(list(worker_item.glob("*.zip")))
                print(f"   ğŸ“ {worker_item.name}: {zip_count}ê°œ ZIP íŒŒì¼")
            else:
                print(f"   ğŸ“¦ {worker_item.stem}: í†µí•© ZIP íŒŒì¼")
        
        # ì „ì²´ ê²°ê³¼ ì €ì¥ìš©
        all_results = {
            "analysis_date": time.strftime("%Y-%m-%d %H:%M:%S"),
            "total_workers": len(worker_folders),
            "workers": {},
            "summary": {}
        }
        
        total_documents = 0
        total_pages = 0
        
        # ê° ì‘ì—…ìë³„ ë¶„ì„
        for worker_item in worker_folders:
            if worker_item.is_dir():
                worker_name = worker_item.name
                worker_zip_source = worker_item  # í´ë” ë‚´ ZIP íŒŒì¼ë“¤
            else:
                worker_name = worker_item.stem
                worker_zip_source = worker_item  # í†µí•© ZIP íŒŒì¼
            
            print(f"\nğŸ‘¤ {worker_name} ë¶„ì„ ì¤‘...")
            
            try:
                # í˜ì´ì§€ ìˆ˜ ê³„ì‚°
                worker_total_pages = 0
                worker_file_pages = {}
                
                if worker_item.is_dir():
                    # í´ë” ë‚´ ZIP íŒŒì¼ë“¤ ì²˜ë¦¬ (í†µí•© ZIP)
                    worker_zip_files = list(worker_zip_source.glob("*.zip"))
                    print(f"   ğŸ“ í´ë”ì—ì„œ {len(worker_zip_files)}ê°œ í†µí•© ZIP íŒŒì¼ ë°œê²¬")
                    
                    # ëª¨ë“  ê°œë³„ ZIP íŒŒì¼ì„ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
                    zip_files = []
                    
                    # ê° í†µí•© ZIP íŒŒì¼ì„ ì²˜ë¦¬
                    for unified_zip in worker_zip_files:
                        print(f"   ğŸ“¦ í†µí•© ZIP í•´ì œ: {unified_zip.name}")
                        
                        # í†µí•© ZIPì„ ì‘ì—…ì í´ë”ì— ì§ì ‘ í•´ì œ
                        unified_extract_dir = worker_zip_source / f"extracted_{unified_zip.stem}"
                        unified_extract_dir.mkdir(exist_ok=True)
                        
                        print(f"      ğŸ“ í•´ì œ ìœ„ì¹˜: {unified_extract_dir}")
                        
                        try:
                            with zipfile.ZipFile(unified_zip, 'r') as zip_ref:
                                file_list = zip_ref.namelist()
                                individual_zips = [f for f in file_list if f.endswith('.zip')]
                                print(f"      ğŸ“¦ ë‚´ë¶€ì—ì„œ {len(individual_zips)}ê°œ ê°œë³„ ZIP ë°œê²¬")
                                
                                # í†µí•© ZIP ì••ì¶• í•´ì œ (ì‘ì—…ì í´ë”ì— ì§ì ‘)
                                zip_ref.extractall(unified_extract_dir)
                                print(f"      âœ… í•´ì œ ì™„ë£Œ: {len(file_list)}ê°œ íŒŒì¼")
                                
                                # ê°œë³„ ZIP íŒŒì¼ë“¤ì„ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
                                for zip_name in individual_zips:
                                    zip_path = unified_extract_dir / zip_name
                                    if zip_path.exists():
                                        zip_files.append(zip_path)
                                        if worker_name in ['ê°•ì•„ë¦„', 'ê¹€ê²½ì§„']:
                                            print(f"         âœ… ê°œë³„ ZIP: {zip_name}")
                                
                        except Exception as e:
                            print(f"      âŒ í†µí•© ZIP í•´ì œ ì˜¤ë¥˜: {e}")
                            continue
                    
                    print(f"   ğŸ“Š ì´ {len(zip_files)}ê°œ ê°œë³„ ZIP íŒŒì¼ ì²˜ë¦¬ ì˜ˆì •")
                else:
                    # í†µí•© ZIP íŒŒì¼ ë¨¼ì € ì••ì¶• í•´ì œ
                    print(f"   ğŸ“¦ í†µí•© ZIP íŒŒì¼ ì••ì¶• í•´ì œ ì¤‘... ({worker_zip_source})")
                    print(f"   ğŸ“¦ íŒŒì¼ í¬ê¸°: {worker_zip_source.stat().st_size:,} bytes")
                    
                    worker_extract_dir = temp_extract_dir / "worker_extracted"
                    worker_extract_dir.mkdir(exist_ok=True)
                    
                    try:
                        with zipfile.ZipFile(worker_zip_source, 'r') as zip_ref:
                            # ZIP íŒŒì¼ ë‚´ìš© í™•ì¸
                            file_list = zip_ref.namelist()
                            print(f"   ğŸ“¦ ZIP ë‚´ë¶€ íŒŒì¼ {len(file_list)}ê°œ ë°œê²¬")
                            if len(file_list) <= 10:
                                for f in file_list[:10]:
                                    print(f"      - {f}")
                            else:
                                for f in file_list[:5]:
                                    print(f"      - {f}")
                                print(f"      ... (ì´ {len(file_list)}ê°œ)")
                            
                            # ì••ì¶• í•´ì œ
                            zip_ref.extractall(worker_extract_dir)
                            print(f"   âœ… ì••ì¶• í•´ì œ ì™„ë£Œ: {worker_extract_dir}")
                            
                    except zipfile.BadZipFile as e:
                        print(f"   âŒ ì˜ëª»ëœ ZIP íŒŒì¼: {e}")
                        continue
                    except Exception as e:
                        print(f"   âŒ ì••ì¶• í•´ì œ ì˜¤ë¥˜: {e}")
                        continue
                    
                    # ì••ì¶• í•´ì œëœ í´ë”ì—ì„œ ê°œë³„ ZIP íŒŒì¼ë“¤ ì°¾ê¸°
                    zip_files = []
                    print(f"   ğŸ” ê°œë³„ ZIP íŒŒì¼ ê²€ìƒ‰ ì¤‘...")
                    
                    for root, dirs, files in os.walk(worker_extract_dir):
                        print(f"      ğŸ“‚ ê²€ìƒ‰ ì¤‘: {root}")
                        zip_in_dir = [f for f in files if f.endswith('.zip')]
                        if zip_in_dir:
                            print(f"         ì°¾ì€ ZIP: {zip_in_dir}")
                        for file in files:
                            if file.endswith('.zip'):
                                zip_path = Path(root) / file
                                zip_files.append(zip_path)
                                print(f"         âœ… ë°œê²¬: {zip_path}")
                    
                    print(f"   ğŸ“¦ í†µí•© ZIPì—ì„œ {len(zip_files)}ê°œ ê°œë³„ ZIP íŒŒì¼ ë°œê²¬")
                
                # ê°œë³„ ZIP íŒŒì¼ë“¤ ë¶„ì„ (ZipProcessor ì‚¬ìš©í•˜ì§€ ì•ŠìŒ)
                for zip_file in zip_files:
                    zip_name = zip_file.stem
                    if zip_name.startswith('visualcontent-'):
                        doc_name = zip_name.replace('visualcontent-', '')
                    else:
                        doc_name = zip_name
                    
                    try:
                        # ê°œë³„ ZIP íŒŒì¼ì„ ì‘ì—…ì í´ë” ë‚´ì— ì••ì¶• í•´ì œ
                        # íŒŒì¼ëª…ì—ì„œ .zip ì œê±°í•˜ê³  í´ë”ëª…ìœ¼ë¡œ ì‚¬ìš©
                        zip_extract_dir = zip_file.parent / f"doc_{doc_name}"
                        zip_extract_dir.mkdir(exist_ok=True)
                        
                        with zipfile.ZipFile(zip_file, 'r') as zip_ref:
                            zip_ref.extractall(zip_extract_dir)
                        
                        # original í´ë”ì˜ PDF íŒŒì¼ ì°¾ê¸°
                        original_folder = zip_extract_dir / "original"
                        
                        if worker_name in ['ê°•ì•„ë¦„', 'ê¹€ê²½ì§„']:  # ì²˜ìŒ 2ëª…ë§Œ ìƒì„¸ ë¡œê·¸
                            print(f"      ğŸ“„ {doc_name} ë¶„ì„:")
                            print(f"         ZIP ì••ì¶• í•´ì œ: {zip_extract_dir}")
                            print(f"         original í´ë” ì¡´ì¬: {original_folder.exists()}")
                        
                        if original_folder.exists():
                            pdf_files = list(original_folder.glob("*.pdf"))
                            if worker_name in ['ê°•ì•„ë¦„', 'ê¹€ê²½ì§„']:
                                print(f"         PDF íŒŒì¼: {[f.name for f in pdf_files]}")
                            
                            if pdf_files:
                                pdf_file = pdf_files[0]
                                try:
                                    with open(pdf_file, 'rb') as file:
                                        pdf_reader = PyPDF2.PdfReader(file)
                                        pages_count = len(pdf_reader.pages)
                                    
                                    if worker_name in ['ê°•ì•„ë¦„', 'ê¹€ê²½ì§„']:
                                        print(f"         PDF í˜ì´ì§€ ìˆ˜: {pages_count}í˜ì´ì§€")
                                        print(f"         PDF íŒŒì¼ í¬ê¸°: {pdf_file.stat().st_size:,} bytes")
                                    
                                    worker_file_pages[doc_name] = pages_count
                                    worker_total_pages += pages_count
                                    
                                except Exception as e:
                                    if worker_name in ['ê°•ì•„ë¦„', 'ê¹€ê²½ì§„']:
                                        print(f"         âŒ PDF ì½ê¸° ì˜¤ë¥˜: {e}")
                                    worker_file_pages[doc_name] = 1
                                    worker_total_pages += 1
                            else:
                                if worker_name in ['ê°•ì•„ë¦„', 'ê¹€ê²½ì§„']:
                                    print(f"         âš ï¸ PDF íŒŒì¼ ì—†ìŒ - ê¸°ë³¸ê°’ 1í˜ì´ì§€ ì ìš©")
                                worker_file_pages[doc_name] = 1
                                worker_total_pages += 1
                        else:
                            if worker_name in ['ê°•ì•„ë¦„', 'ê¹€ê²½ì§„']:
                                print(f"         âš ï¸ original í´ë” ì—†ìŒ - ê¸°ë³¸ê°’ 1í˜ì´ì§€ ì ìš©")
                            worker_file_pages[doc_name] = 1
                            worker_total_pages += 1
                            
                    except Exception as e:
                        print(f"âš ï¸ {zip_file.name} ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                        worker_file_pages[doc_name] = 1
                        worker_total_pages += 1
                
                # ì‘ì—…ìë³„ ê²°ê³¼ ì €ì¥
                worker_result = {
                    "documents_count": len(worker_file_pages),
                    "total_pages": worker_total_pages,
                    "average_pages": round(worker_total_pages/len(worker_file_pages), 1) if worker_file_pages else 0,
                    "documents": worker_file_pages
                }
                
                all_results["workers"][worker_name] = worker_result
                total_documents += len(worker_file_pages)
                total_pages += worker_total_pages
                
                # ì‘ì—…ìë³„ ìš”ì•½ ì¶œë ¥
                print(f"   ğŸ“š ë¬¸ì„œ ìˆ˜: {len(worker_file_pages)}ê°œ")
                print(f"   ğŸ“– ì´ í˜ì´ì§€: {worker_total_pages}í˜ì´ì§€")
                print(f"   ğŸ“Š í‰ê· : {worker_total_pages/len(worker_file_pages):.1f}í˜ì´ì§€" if worker_file_pages else "   ğŸ“Š í‰ê· : 0í˜ì´ì§€")
                
            except Exception as e:
                print(f"âŒ {worker_name} ë¶„ì„ ì˜¤ë¥˜: {e}")
                all_results["workers"][worker_name] = {
                    "error": str(e),
                    "documents_count": 0,
                    "total_pages": 0,
                    "average_pages": 0
                }
            
            except Exception as e:
                print(f"âŒ {worker_name} ë¶„ì„ ì˜¤ë¥˜: {e}")
                all_results["workers"][worker_name] = {
                    "error": str(e),
                    "documents_count": 0,
                    "total_pages": 0,
                    "average_pages": 0
                }
        
        # ì „ì²´ ìš”ì•½ ê³„ì‚°
        all_results["summary"] = {
            "total_documents": total_documents,
            "total_pages": total_pages,
            "average_pages_per_document": round(total_pages/total_documents, 1) if total_documents else 0,
            "average_documents_per_worker": round(total_documents/len(worker_folders), 1),
            "average_pages_per_worker": round(total_pages/len(worker_folders), 1)
        }
        
        # ì „ì²´ ê²°ê³¼ ì¶œë ¥
        print(f"\n" + "="*60)
        print(f"ğŸ“Š ì‘ì—…ìë³„ í˜ì´ì§€ ìˆ˜ ë¶„ì„ ì™„ë£Œ")
        print(f"="*60)
        
        for worker_name, result in all_results["workers"].items():
            if "error" not in result:
                print(f"ğŸ‘¤ {worker_name:12} | ğŸ“š {result['documents_count']:3}ê°œ | ğŸ“– {result['total_pages']:4}í˜ì´ì§€ | ğŸ“Š í‰ê·  {result['average_pages']:4.1f}í˜ì´ì§€")
            else:
                print(f"ğŸ‘¤ {worker_name:12} | âŒ ì˜¤ë¥˜ ë°œìƒ")
        
        print(f"="*60)
        print(f"ğŸ“ˆ ì „ì²´ ìš”ì•½:")
        print(f"   ğŸ‘¥ ì´ ì‘ì—…ì: {len(worker_folders)}ëª…")
        print(f"   ğŸ“š ì´ ë¬¸ì„œ: {total_documents}ê°œ")
        print(f"   ğŸ“– ì´ í˜ì´ì§€: {total_pages}í˜ì´ì§€")
        print(f"   ğŸ“Š ë¬¸ì„œë‹¹ í‰ê· : {all_results['summary']['average_pages_per_document']}í˜ì´ì§€")
        print(f"   ğŸ“Š ì‘ì—…ìë‹¹ í‰ê·  ë¬¸ì„œ: {all_results['summary']['average_documents_per_worker']}ê°œ")
        print(f"   ğŸ“Š ì‘ì—…ìë‹¹ í‰ê·  í˜ì´ì§€: {all_results['summary']['average_pages_per_worker']}í˜ì´ì§€")
        
        # ë³´ê³ ì„œ ì €ì¥
        if args.report:
            with open(args.report, 'w', encoding='utf-8') as f:
                json.dump(all_results, f, ensure_ascii=False, indent=2)
            print(f"\nğŸ“„ ì‘ì—…ìë³„ ë¶„ì„ ë³´ê³ ì„œ ì €ì¥: {args.report}")
        
        return
    
    # í˜ì´ì§€ ìˆ˜ ê³„ì‚° ëª¨ë“œ
    if args.count_pages:
        print("ğŸ“Š í˜ì´ì§€ ìˆ˜ ë¶„ì„ ì‹œì‘")
        
        # ì„ì‹œ ì¶”ì¶œ ë””ë ‰í† ë¦¬ ìƒì„±
        temp_extract_dir = target_path / "temp_page_count"
        temp_extract_dir.mkdir(exist_ok=True)
        
        try:
            # ZIP íŒŒì¼ ì¶”ì¶œ
            processor = ZipProcessor(extract_base_dir=temp_extract_dir)
            json_files = processor.process_directory(target_path)

            if not json_files:
                print("âŒ JSON íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                shutil.rmtree(temp_extract_dir, ignore_errors=True)
                sys.exit(1)

            # í˜ì´ì§€ ìˆ˜ ê³„ì‚° - PDF íŒŒì¼ ê¸°ë°˜ìœ¼ë¡œ ì •í™•íˆ ê³„ì‚°
            total_pages = 0
            file_pages = {}
            zip_files = list(target_path.glob("*.zip"))
            
            print(f"\nğŸ“‚ ë¶„ì„ ëŒ€ìƒ: {len(zip_files)}ê°œ ZIP íŒŒì¼")
            
            # ê° ZIP íŒŒì¼ì˜ ì¶”ì¶œëœ í´ë”ì—ì„œ PDF íŒŒì¼ ì°¾ê¸°
            for zip_file in zip_files:
                zip_name = zip_file.stem
                if zip_name.startswith('visualcontent-'):
                    doc_name = zip_name.replace('visualcontent-', '')
                else:
                    doc_name = zip_name
                
                # ì¶”ì¶œëœ í´ë”ì—ì„œ original í´ë”ì˜ PDF íŒŒì¼ ì°¾ê¸°
                extracted_folder = temp_extract_dir / zip_name
                original_folder = extracted_folder / "original"
                
                if original_folder.exists():
                    pdf_files = list(original_folder.glob("*.pdf"))
                    if pdf_files:
                        pdf_file = pdf_files[0]  # ì²« ë²ˆì§¸ PDF íŒŒì¼ ì‚¬ìš©
                        try:
                            # PDF íŒŒì¼ì˜ í˜ì´ì§€ ìˆ˜ ì½ê¸°
                            with open(pdf_file, 'rb') as file:
                                pdf_reader = PyPDF2.PdfReader(file)
                                pages_count = len(pdf_reader.pages)
                                
                            file_pages[doc_name] = pages_count
                            total_pages += pages_count
                            
                        except Exception as e:
                            print(f"âš ï¸ {pdf_file.name} PDF ì½ê¸° ì˜¤ë¥˜: {e}")
                            # PDF ì½ê¸° ì‹¤íŒ¨ì‹œ JSON ê¸°ë°˜ìœ¼ë¡œ ì¶”ì •
                            file_pages[doc_name] = 1
                            total_pages += 1
                    else:
                        print(f"âš ï¸ {doc_name}: original í´ë”ì— PDF íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤")
                        file_pages[doc_name] = 1
                        total_pages += 1
                else:
                    print(f"âš ï¸ {doc_name}: original í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                    file_pages[doc_name] = 1
                    total_pages += 1

            # ê²°ê³¼ ì¶œë ¥
            print(f"\nğŸ“Š í˜ì´ì§€ ìˆ˜ ë¶„ì„ ê²°ê³¼:")
            print(f"=" * 50)
            
            for file_name, pages in sorted(file_pages.items()):
                print(f"ğŸ“„ {file_name}: {pages}í˜ì´ì§€")
            
            print(f"=" * 50)
            print(f"ğŸ“š ì´ {len(file_pages)}ê°œ ë¬¸ì„œ")
            print(f"ğŸ“– ì´ í˜ì´ì§€ ìˆ˜: {total_pages}í˜ì´ì§€")
            print(f"ğŸ“Š í‰ê·  í˜ì´ì§€ ìˆ˜: {total_pages/len(file_pages):.1f}í˜ì´ì§€" if file_pages else "ğŸ“Š í‰ê· : 0í˜ì´ì§€")
            
            # ë³´ê³ ì„œ ì €ì¥
            if args.report:
                page_report = {
                    "analysis_date": time.strftime("%Y-%m-%d %H:%M:%S"),
                    "total_documents": len(file_pages),
                    "total_pages": total_pages,
                    "average_pages": round(total_pages/len(file_pages), 1) if file_pages else 0,
                    "documents": file_pages
                }
                
                with open(args.report, 'w', encoding='utf-8') as f:
                    json.dump(page_report, f, ensure_ascii=False, indent=2)
                print(f"ğŸ“„ í˜ì´ì§€ ìˆ˜ ë³´ê³ ì„œ ì €ì¥: {args.report}")
        
        finally:
            # ì„ì‹œ ë””ë ‰í† ë¦¬ ì •ë¦¬
            shutil.rmtree(temp_extract_dir, ignore_errors=True)
        
        return
    
    # PDF ì—…ë¡œë“œ ë° OCR ì²˜ë¦¬ ëª¨ë“œ
    if args.upload:
        print("ğŸ“¤ PDF ì—…ë¡œë“œ ë° OCR ì²˜ë¦¬ ì‹œì‘")
        
        # PDF íŒŒì¼ ê²€ìƒ‰
        pdf_files = list(target_path.glob("*.pdf")) + list(target_path.glob("*.PDF"))
        
        if not pdf_files:
            print("âŒ PDF íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            sys.exit(1)
        
        print(f"ğŸ“‚ ë°œê²¬ëœ PDF íŒŒì¼: {len(pdf_files)}ê°œ")
        
        # PDF ì—…ë¡œë“œ ì²˜ë¦¬ê¸° ì´ˆê¸°í™”
        uploader = PDFUploader()
        
        success_count = 0
        start_time = time.time()
        
        # tqdmìœ¼ë¡œ ì§„í–‰ë¥  í‘œì‹œ
        with tqdm(total=len(pdf_files), desc="ğŸ“¤ PDF ì²˜ë¦¬ ì¤‘", 
                  unit="íŒŒì¼", bar_format="{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]") as pbar:
            
            for pdf_file in pdf_files:
                file_start_time = time.time()
                pbar.set_postfix_str(f"í˜„ì¬: {pdf_file.name}")
                
                try:
                    # 1. PDF ì—…ë¡œë“œ
                    pbar.set_description("ğŸ“¤ ì—…ë¡œë“œ ì¤‘")
                    file_info = uploader.upload_pdf(pdf_file)
                    if not file_info:
                        pbar.write(f"âŒ ì—…ë¡œë“œ ì‹¤íŒ¨: {pdf_file.name}")
                        pbar.update(1)
                        continue
                    
                    upload_time = time.time() - file_start_time
                    pbar.write(f"âœ… ì—…ë¡œë“œ ì™„ë£Œ: {file_info['fileName']} ({file_info['numOfPages']}í˜ì´ì§€, {upload_time:.1f}ì´ˆ)")
                    
                    # 2. OCR ì²˜ë¦¬ ìš”ì²­
                    pbar.set_description("ğŸ” OCR ìš”ì²­ ì¤‘")
                    ocr_start_time = time.time()
                    extract_result = uploader.extract_pages(file_info['fileId'], f"1-{file_info['numOfPages']}")
                    if not extract_result:
                        pbar.write(f"âŒ OCR ì²˜ë¦¬ ì‹¤íŒ¨: {pdf_file.name}")
                        pbar.update(1)
                        continue
                    
                    ocr_request_time = time.time() - ocr_start_time
                    pbar.write(f"ğŸ” OCR ìš”ì²­ ì™„ë£Œ: {pdf_file.name} ({ocr_request_time:.1f}ì´ˆ)")
                    
                    # 3. VisualInfo ë‹¤ìš´ë¡œë“œ (ëŒ€ê¸° ì‹œê°„ í¬í•¨)
                    pbar.set_description("â³ OCR ê²°ê³¼ ëŒ€ê¸°")
                    visual_start_time = time.time()
                    visual_info = uploader.get_visual_info(file_info['fileId'], progress_callback=lambda msg: pbar.set_postfix_str(msg))
                    if not visual_info:
                        pbar.write(f"âŒ VisualInfo ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {pdf_file.name}")
                        pbar.update(1)
                        continue
                    
                    visual_time = time.time() - visual_start_time
                    pbar.write(f"ğŸ“„ VisualInfo ì™„ë£Œ: {pdf_file.name} ({visual_time:.1f}ì´ˆ)")
                    
                    # 4. ZIP íŒŒì¼ë¡œ ì €ì¥
                    pbar.set_description("ğŸ“¦ ZIP ìƒì„± ì¤‘")
                    zip_start_time = time.time()
                    zip_filename = f"visualcontent-{pdf_file.stem}.zip"
                    zip_path = target_path / zip_filename
                    
                    zip_created = uploader.create_visualcontent_zip(
                        pdf_file, visual_info, zip_path, file_info['fileId']
                    )
                    
                    zip_time = time.time() - zip_start_time
                    file_total_time = time.time() - file_start_time
                    
                    if zip_created:
                        pbar.write(f"ğŸ“¦ ZIP ìƒì„± ì™„ë£Œ: {zip_filename} (ìƒì„±: {zip_time:.1f}ì´ˆ, ì´: {file_total_time:.1f}ì´ˆ)")
                        success_count += 1
                    else:
                        pbar.write(f"âŒ ZIP íŒŒì¼ ìƒì„± ì‹¤íŒ¨: {pdf_file.name}")
                    
                except Exception as e:
                    pbar.write(f"âŒ {pdf_file.name} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                finally:
                    pbar.update(1)
        
        total_time = time.time() - start_time
        print(f"\nâœ… ì—…ë¡œë“œ ì™„ë£Œ: {success_count}/{len(pdf_files)}ê°œ íŒŒì¼")
        print(f"â±ï¸  ì´ ì²˜ë¦¬ ì‹œê°„: {total_time:.1f}ì´ˆ (í‰ê· : {total_time/len(pdf_files):.1f}ì´ˆ/íŒŒì¼)")
        return
    
    # ListText í†µì¼ + R003 ë²•ë ¹ êµ¬ì¡° ì ìš© ëª¨ë“œ
    if args.listtext_only:
        print("ğŸ“ ListText í†µì¼ + R003 ë²•ë ¹ êµ¬ì¡° ì ìš© ì‹œì‘")
        
        # ê²€ìˆ˜ í´ë” ë° ë‚´ë¶€ extracted_data í´ë” ìƒì„± (ZIP íŒŒì¼ì´ ìˆëŠ” ë””ë ‰í† ë¦¬ì—)
        review_dir = target_path / "ê²€ìˆ˜_ListText"
        review_dir.mkdir(exist_ok=True)
        extracted_dir = review_dir / "extracted_data"
        extracted_dir.mkdir(exist_ok=True)
        print(f"ğŸ“ ê²€ìˆ˜ í´ë” ìƒì„±: {review_dir}")
        print(f"ğŸ“ ì¶”ì¶œ í´ë” ìƒì„±: {extracted_dir}")

        # 1. ZIP íŒŒì¼ ì¶”ì¶œ
        processor = ZipProcessor(extract_base_dir=extracted_dir)
        json_files = processor.process_directory(target_path)

        if not json_files:
            print("âŒ JSON íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            sys.exit(1)

        # 2. ListText í†µì¼ + R003 ì ìš©
        print(f"\nğŸ”§ ListText í†µì¼ ë° R003 ë²•ë ¹ êµ¬ì¡° ì ìš©: {len(json_files)}ê°œ íŒŒì¼")
        total_changes = 0

        for json_file in json_files:
            if "visualinfo" in json_file.name:
                try:
                    # JSON íŒŒì¼ ë¡œë“œ
                    with open(json_file, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                    
                    elements = data.get('elements', [])
                    changes = 0
                    
                    for element in elements:
                        category = element.get('category', {})
                        text = element.get('content', {}).get('text', '').strip()
                        current_label = category.get('label', '')
                        
                        # 1ë‹¨ê³„: PageNumberë¥¼ ì œì™¸í•˜ê³  ëª¨ë“  ê²ƒì„ ListTextë¡œ ë³€ê²½
                        if current_label != 'ListText' and current_label != 'PageNumber':
                            category['label'] = 'ListText'
                            category['type'] = 'LIST'  # typeë„ í•¨ê»˜ ë³€ê²½
                            changes += 1
                    
                    # 2ë‹¨ê³„: R003 ë²•ë ¹ êµ¬ì¡° ê·œì¹™ ì ìš© (ListText â†’ ParaTitle)
                    for element in elements:
                        category = element.get('category', {})
                        text = element.get('content', {}).get('text', '').strip()
                        current_label = category.get('label', '')
                        
                        if current_label == 'ListText' and text:
                            # ~ë²• íŒ¨í„´
                            if re.search(r'[ê°€-í£]+ë²•$', text):
                                category['label'] = 'ParaTitle'
                                category['type'] = 'HEADING'  # typeë„ í•¨ê»˜ ë³€ê²½
                                changes += 1
                            # ì œ~í¸, ì œ~ì¥, ì œ~ì ˆ, ì œ~ì¡° íŒ¨í„´
                            elif re.search(r'^ì œ\s*\d+\s*(í¸|ì¥|ì ˆ|ê´€|ì¡°)', text):
                                category['label'] = 'ParaTitle'
                                category['type'] = 'HEADING'  # typeë„ í•¨ê»˜ ë³€ê²½
                                changes += 1
                    
                    if changes > 0:
                        # JSON íŒŒì¼ ì €ì¥
                        with open(json_file, 'w', encoding='utf-8') as f:
                            json.dump(data, f, ensure_ascii=False, indent=2)
                        
                        total_changes += changes
                        print(f"  âœ… {json_file.parent.parent.name}: {changes}ê°œ ë³€ê²½")
                    else:
                        print(f"  ğŸ“ {json_file.parent.parent.name}: ë³€ê²½í•  í•­ëª© ì—†ìŒ")
                        
                except Exception as e:
                    print(f"  âŒ {json_file.parent.parent.name}: ì˜¤ë¥˜ - {str(e)}")

        print(f"\nğŸ“Š ì´ ë³€ê²½ í•­ëª©: {total_changes}ê°œ")

        # 3. ì¬ì••ì¶•
        if total_changes > 0:
            print(f"\nğŸ“¦ ì¬ì••ì¶• ì¤‘...")
            recompressor = ZipRecompressor(review_dir)
            recompressed_files = recompressor.recompress_all(extracted_dir)

            print(f"âœ… ì²˜ë¦¬ ì™„ë£Œ!")
            print(f"ğŸ“ ìˆ˜ì •ëœ íŒŒì¼ ì €ì¥ ìœ„ì¹˜: {review_dir}")
            print(f"ğŸ“¦ ìƒì„±ëœ íŒŒì¼: {len(recompressed_files)}ê°œ")
        else:
            print("ğŸ“ ë³€ê²½ì‚¬í•­ì´ ì—†ì–´ ì¬ì••ì¶•í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

        return
    
    if args.listtext_only2:
        print("ğŸ“ ListText í†µì¼ + R003 ë²•ë ¹ êµ¬ì¡° ì ìš© (í´ë” êµ¬ì¡° ìœ ì§€)")
        
        # 1. ê²½ë¡œ ì„¤ì • ë° ì…ë ¥ íƒ€ì… í™•ì¸
        if target_path.is_file() and target_path.suffix.lower() == '.zip':
            # ë‹¨ì¼ ZIP íŒŒì¼ ì²˜ë¦¬
            output_dir = target_path.parent / f"{target_path.stem}_ListText"
            temp_processing_dir = target_path.parent / "temp_processing_ListText"
            zip_files = [target_path]
            print(f"ğŸ“„ ë‹¨ì¼ ZIP íŒŒì¼ ì²˜ë¦¬ ëª¨ë“œ")
        else:
            # í´ë” ì²˜ë¦¬ (ê¸°ì¡´ ë°©ì‹)
            output_dir = target_path.parent / f"{target_path.name}_ListText"
            temp_processing_dir = target_path.parent / "temp_processing_ListText"
            zip_files = list(target_path.rglob("*.zip"))
            print(f"ğŸ“ í´ë” ì²˜ë¦¬ ëª¨ë“œ")
        
        # ì´ì „ ê²°ê³¼ í´ë”/ì„ì‹œ í´ë” ì‚­ì œ
        if output_dir.exists():
            safe_rmtree(output_dir)
        if temp_processing_dir.exists():
            safe_rmtree(temp_processing_dir)
            
        output_dir.mkdir(exist_ok=True)
        temp_processing_dir.mkdir(exist_ok=True)
        
        print(f"ğŸ“‚ ì›ë³¸ ê²½ë¡œ: {target_path}")
        print(f"ğŸ“ ê²°ê³¼ë¬¼ ì €ì¥ ê²½ë¡œ: {output_dir}")
        
        # 2. ZIP íŒŒì¼ ëª©ë¡ í™•ì¸
        if not zip_files:
            if target_path.is_file():
                print("âŒ ì§€ì •ëœ íŒŒì¼ì´ ZIP íŒŒì¼ì´ ì•„ë‹™ë‹ˆë‹¤.")
            else:
                print("âŒ ì²˜ë¦¬í•  ZIP íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            safe_rmtree(temp_processing_dir)
            sys.exit(1)
            
        print(f"ï¿½ ì´ {len(zip_files)}ê°œì˜ ZIP íŒŒì¼ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.")
        
        total_changes = 0
        
        # 3. íŒŒì¼ ë‹¨ìœ„ ì²˜ë¦¬
        with tqdm(total=len(zip_files), desc="ğŸš€ ì „ì²´ ì§„í–‰ë¥ ", unit="ê°œ") as pbar:
            for zip_path in zip_files:
                pbar.set_postfix_str(zip_path.name)
                
                try:
                    # 3-1. ì„ì‹œ ì¶”ì¶œ
                    extract_dir = temp_processing_dir / zip_path.stem
                    with zipfile.ZipFile(zip_path, 'r') as zip_ref:
                        zip_ref.extractall(extract_dir)
                        
                    # 3-2. visualinfo.json íŒŒì¼ ì°¾ê¸°
                    visualinfo_files = list(extract_dir.rglob("*_visualinfo.json"))
                    if not visualinfo_files:
                        pbar.write(f"  âš ï¸ {zip_path.name}: visualinfo.json íŒŒì¼ ì—†ìŒ")
                        # ì›ë³¸ ZIP ê·¸ëŒ€ë¡œ ë³µì‚¬
                        relative_path = zip_path.relative_to(target_path)
                        dest_path = output_dir / relative_path
                        dest_path.parent.mkdir(parents=True, exist_ok=True)
                        shutil.copy2(zip_path, dest_path)
                        continue

                    json_file = visualinfo_files[0]
                    
                    # 3-3. ListText ë³€í™˜ ë¡œì§
                    with open(json_file, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                    
                    elements = data.get('elements', [])
                    changes = 0
                    
                    # 1ë‹¨ê³„: PageNumber ì œì™¸í•˜ê³  ListTextë¡œ ë³€ê²½
                    for element in elements:
                        category = element.get('category', {})
                        if category.get('label') not in ['ListText', 'PageNumber']:
                            category['label'] = 'ListText'
                            category['type'] = 'LIST'
                            changes += 1
                    
                    # 2ë‹¨ê³„: ë²•ë ¹ êµ¬ì¡° ê·œì¹™ ì ìš©
                    for element in elements:
                        category = element.get('category', {})
                        text = element.get('content', {}).get('text', '').strip()
                        if category.get('label') == 'ListText' and text:
                            if re.search(r'[ê°€-í£]+ë²•$', text) or re.search(r'^ì œ\s*\d+\s*(í¸|ì¥|ì ˆ|ê´€|ì¡°)', text):
                                category['label'] = 'ParaTitle'
                                category['type'] = 'HEADING'
                                changes += 1
                    
                    if changes > 0:
                        with open(json_file, 'w', encoding='utf-8') as f:
                            json.dump(data, f, ensure_ascii=False, indent=2)
                        total_changes += changes
                        pbar.write(f"  âœ… {zip_path.name}: {changes}ê°œ í•­ëª© ë³€ê²½")
                    else:
                        pbar.write(f"  ğŸ“ {zip_path.name}: ë³€ê²½ ì‚¬í•­ ì—†ìŒ")

                    # 3-4. ê²°ê³¼ë¬¼ ì¬ì••ì¶• ë° ì €ì¥
                    relative_path = zip_path.relative_to(target_path)
                    dest_path = output_dir / relative_path
                    dest_path.parent.mkdir(parents=True, exist_ok=True)
                    
                    with zipfile.ZipFile(dest_path, 'w', zipfile.ZIP_DEFLATED) as new_zip:
                        for file_to_zip in extract_dir.rglob('*'):
                            if file_to_zip.is_file():
                                arcname = file_to_zip.relative_to(extract_dir)
                                new_zip.write(file_to_zip, arcname)

                except Exception as e:
                    pbar.write(f"  âŒ {zip_path.name} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                finally:
                    # ì„ì‹œ ì¶”ì¶œ í´ë” ì •ë¦¬
                    if 'extract_dir' in locals():
                        safe_rmtree(extract_dir)
                    pbar.update(1)

        # 4. ìµœì¢… ì •ë¦¬
        safe_rmtree(temp_processing_dir)
        print(f"\nğŸ‰ ì²˜ë¦¬ ì™„ë£Œ! ì´ {total_changes}ê°œ í•­ëª©ì´ ë³€ê²½ë˜ì—ˆìŠµë‹ˆë‹¤.")
        print(f"ğŸ“‚ ê²°ê³¼ëŠ” {output_dir} í´ë”ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        return
    
    if args.paratext_only:
        print("ğŸ“ ListText í†µì¼ + R003 ë²•ë ¹ êµ¬ì¡° ì ìš© (í´ë” êµ¬ì¡° ìœ ì§€)")
        
        # 1. ê²½ë¡œ ì„¤ì •
        output_dir = target_path.parent / f"{target_path.name}_ListText"
        temp_processing_dir = target_path.parent / "temp_processing_ListText"
        
        # ì´ì „ ê²°ê³¼ í´ë”/ì„ì‹œ í´ë” ì‚­ì œ
        if output_dir.exists():
            shutil.rmtree(output_dir)
        if temp_processing_dir.exists():
            shutil.rmtree(temp_processing_dir)
            
        output_dir.mkdir(exist_ok=True)
        temp_processing_dir.mkdir(exist_ok=True)
        
        print(f"ï¿½ ì›ë³¸ ê²½ë¡œ: {target_path}")
        print(f"ğŸ“ ê²°ê³¼ë¬¼ ì €ì¥ ê²½ë¡œ: {output_dir}")
        
        # 2. ZIP íŒŒì¼ ëª©ë¡ íƒìƒ‰
        zip_files = list(target_path.rglob("*.zip"))
        if not zip_files:
            print("âŒ ì²˜ë¦¬í•  ZIP íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            shutil.rmtree(temp_processing_dir)
            sys.exit(1)
            
        print(f"ï¿½ ì´ {len(zip_files)}ê°œì˜ ZIP íŒŒì¼ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.")
        
        total_changes = 0
        
        # 3. íŒŒì¼ ë‹¨ìœ„ ì²˜ë¦¬
        with tqdm(total=len(zip_files), desc="ğŸš€ ì „ì²´ ì§„í–‰ë¥ ", unit="ê°œ") as pbar:
            for zip_path in zip_files:
                pbar.set_postfix_str(zip_path.name)
                
                try:
                    # 3-1. ì„ì‹œ ì¶”ì¶œ
                    extract_dir = temp_processing_dir / zip_path.stem
                    with zipfile.ZipFile(zip_path, 'r') as zip_ref:
                        zip_ref.extractall(extract_dir)
                        
                    # 3-2. visualinfo.json íŒŒì¼ ì°¾ê¸°
                    visualinfo_files = list(extract_dir.rglob("*_visualinfo.json"))
                    if not visualinfo_files:
                        pbar.write(f"  âš ï¸ {zip_path.name}: visualinfo.json íŒŒì¼ ì—†ìŒ")
                        # ì›ë³¸ ZIP ê·¸ëŒ€ë¡œ ë³µì‚¬
                        relative_path = zip_path.relative_to(target_path)
                        dest_path = output_dir / relative_path
                        dest_path.parent.mkdir(parents=True, exist_ok=True)
                        shutil.copy2(zip_path, dest_path)
                        continue

                    json_file = visualinfo_files[0]
                    
                    # 3-3. ListText ë³€í™˜ ë¡œì§
                    with open(json_file, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                    
                    elements = data.get('elements', [])
                    changes = 0
                    
                    # 1ë‹¨ê³„: PageNumber ì œì™¸í•˜ê³  ListTextë¡œ ë³€ê²½
                    for element in elements:
                        category = element.get('category', {})
                        if category.get('label') in ['ParaText']:
                            category['label'] = 'ListText'
                            category['type'] = 'LIST'
                            changes += 1
                    
                    # 2ë‹¨ê³„: ë²•ë ¹ êµ¬ì¡° ê·œì¹™ ì ìš©
                    for element in elements:
                        category = element.get('category', {})
                        text = element.get('content', {}).get('text', '').strip()
                        if category.get('label') == 'ListText' and text:
                            if re.search(r'[ê°€-í£]+ë²•$', text) or re.search(r'^ì œ\s*\d+\s*(í¸|ì¥|ì ˆ|ê´€|ì¡°)', text):
                                category['label'] = 'ParaTitle'
                                category['type'] = 'HEADING'
                                changes += 1
                    
                    if changes > 0:
                        with open(json_file, 'w', encoding='utf-8') as f:
                            json.dump(data, f, ensure_ascii=False, indent=2)
                        total_changes += changes
                        pbar.write(f"  âœ… {zip_path.name}: {changes}ê°œ í•­ëª© ë³€ê²½")
                    else:
                        pbar.write(f"  ğŸ“ {zip_path.name}: ë³€ê²½ ì‚¬í•­ ì—†ìŒ")

                    # 3-4. ê²°ê³¼ë¬¼ ì¬ì••ì¶• ë° ì €ì¥
                    relative_path = zip_path.relative_to(target_path)
                    dest_path = output_dir / relative_path
                    dest_path.parent.mkdir(parents=True, exist_ok=True)
                    
                    with zipfile.ZipFile(dest_path, 'w', zipfile.ZIP_DEFLATED) as new_zip:
                        for file_to_zip in extract_dir.rglob('*'):
                            if file_to_zip.is_file():
                                arcname = file_to_zip.relative_to(extract_dir)
                                new_zip.write(file_to_zip, arcname)

                except Exception as e:
                    pbar.write(f"  âŒ {zip_path.name} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                finally:
                    # ì„ì‹œ ì¶”ì¶œ í´ë” ì •ë¦¬
                    shutil.rmtree(extract_dir, ignore_errors=True)
                    pbar.update(1)

        # 4. ìµœì¢… ì •ë¦¬
        shutil.rmtree(temp_processing_dir, ignore_errors=True)
        print(f"\nğŸ‰ ì²˜ë¦¬ ì™„ë£Œ! ì´ {total_changes}ê°œ í•­ëª©ì´ ë³€ê²½ë˜ì—ˆìŠµë‹ˆë‹¤.")
        print(f"ï¿½ ê²°ê³¼ëŠ” {output_dir} í´ë”ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        return
    
    # ì§€ì • í´ë”ì— ê²€ìˆ˜ í´ë” ìƒì„±í•˜ì—¬ ì²˜ë¦¬
    if args.process_to_review:
        print("ğŸš€ ê²€ìˆ˜ í´ë” ìƒì„± ë° ìë™ ìˆ˜ì • ì‹œì‘")
        
        # ê²€ìˆ˜ í´ë” ë° ë‚´ë¶€ extracted_data í´ë” ìƒì„±
        review_dir = target_path / "ê²€ìˆ˜"
        review_dir.mkdir(exist_ok=True)
        extracted_dir = review_dir / "extracted_data"
        extracted_dir.mkdir(exist_ok=True)
        print(f"ğŸ“ ê²€ìˆ˜ í´ë” ìƒì„±: {review_dir}")
        print(f"ğŸ“ ì¶”ì¶œ í´ë” ìƒì„±: {extracted_dir}")

        # 1. ZIP íŒŒì¼ ì¶”ì¶œ (ê²€ìˆ˜í´ë” ë‚´ë¶€ extracted_dataì— ì¶”ì¶œ)
        processor = ZipProcessor(extract_base_dir=extracted_dir)
        json_files = processor.process_directory(target_path)

        if not json_files:
            print("âŒ JSON íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            sys.exit(1)

        # 2. ìë™ ìˆ˜ì • (extracted_dir ë‚´ë¶€ íŒŒì¼ë§Œ ëŒ€ìƒìœ¼ë¡œ)
        print(f"\nğŸ”§ ìë™ ìˆ˜ì • ì‹œì‘: {len(json_files)}ê°œ íŒŒì¼")
        total_fixes = 0

        for json_file in json_files:
            if "visualinfo" in json_file.name:
                extract_dir = json_file.parent.parent
                fixer = RuleBasedFixer(extract_dir)
                fixes = fixer.run_all_rule_fixes()

                fix_count = sum(len(fix_list) for fix_list in fixes.values())
                if fix_count > 0:
                    fixer.save_fixes()
                    total_fixes += fix_count
                    print(f"  âœ… {json_file.parent.parent.name}: {fix_count}ê°œ ìˆ˜ì •")
                else:
                    print(f"  ğŸ“ {json_file.parent.parent.name}: ìˆ˜ì •í•  í•­ëª© ì—†ìŒ")

        print(f"\nğŸ“Š ì´ ìˆ˜ì • í•­ëª©: {total_fixes}ê°œ")

        # 3. ê²€ìˆ˜ í´ë”ì— ì¬ì••ì¶• (extracted_dirì—ì„œ review_dirë¡œ)
        if total_fixes > 0:
            print(f"\nğŸ“¦ ê²€ìˆ˜ í´ë”ì— ì¬ì••ì¶• ì¤‘...")
            recompressor = ZipRecompressor(review_dir)
            recompressed_files = recompressor.recompress_all(extracted_dir)

            print(f"âœ… ì²˜ë¦¬ ì™„ë£Œ!")
            print(f"ğŸ“ ìˆ˜ì •ëœ íŒŒì¼ ì €ì¥ ìœ„ì¹˜: {review_dir}")
            print(f"ğŸ“¦ ìƒì„±ëœ íŒŒì¼: {len(recompressed_files)}ê°œ")

            # íŒŒì¼ëª…ì—ì„œ _fixed ì œê±°í•˜ì—¬ ì›ë³¸ ì´ë¦„ìœ¼ë¡œ ë³€ê²½
            for zip_file in recompressed_files:
                original_name = zip_file.name.replace("_fixed.zip", ".zip")
                new_path = zip_file.parent / original_name

                # ê¸°ì¡´ íŒŒì¼ì´ ìˆìœ¼ë©´ ì‚­ì œ
                if new_path.exists():
                    new_path.unlink()

                zip_file.rename(new_path)
                print(f"  ğŸ“„ {original_name}")
        else:
            print("ğŸ“ ìˆ˜ì •ëœ í•­ëª©ì´ ì—†ì–´ ì¬ì••ì¶•ì„ ê±´ë„ˆëœë‹ˆë‹¤.")

        return
    
    # ì „ì²´ ì›Œí¬í”Œë¡œìš° ëª¨ë“œ (ì¶”ì¶œ â†’ ìë™ìˆ˜ì • â†’ ì¬ì••ì¶•)
    if args.full_workflow:
        print("ğŸš€ ì „ì²´ ì›Œí¬í”Œë¡œìš° ì‹œì‘: ì¶”ì¶œ â†’ ìë™ìˆ˜ì • â†’ ì¬ì••ì¶•")
        
        # 1. ZIP íŒŒì¼ ì¶”ì¶œ
        processor = ZipProcessor()
        json_files = processor.process_directory(target_path)
        
        if not json_files:
            print("âŒ JSON íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            sys.exit(1)
        
        # 2. ìë™ ìˆ˜ì •
        print(f"\nğŸ”§ ìë™ ìˆ˜ì • ì‹œì‘: {len(json_files)}ê°œ íŒŒì¼")
        total_fixes = 0
        
        for json_file in json_files:
            if "visualinfo" in json_file.name:
                extract_dir = json_file.parent.parent
                fixer = RuleBasedFixer(extract_dir)
                fixes = fixer.run_all_rule_fixes()
                
                fix_count = sum(len(fix_list) for fix_list in fixes.values())
                if fix_count > 0:
                    fixer.save_fixes()
                    total_fixes += fix_count
                    print(f"  âœ… {json_file.parent.parent.name}: {fix_count}ê°œ ìˆ˜ì •")
                else:
                    print(f"  ğŸ“ {json_file.parent.parent.name}: ìˆ˜ì •í•  í•­ëª© ì—†ìŒ")
        
        print(f"\nğŸ“Š ì´ ìˆ˜ì • í•­ëª©: {total_fixes}ê°œ")
        
        # 3. ì¬ì••ì¶•
        if total_fixes > 0:
            print(f"\nğŸ“¦ ì¬ì••ì¶• ì‹œì‘...")
            recompressor = ZipRecompressor()
            recompressed_files = recompressor.recompress_all(Path("extracted_data"))
            
            print(f"âœ… ì›Œí¬í”Œë¡œìš° ì™„ë£Œ!")
            print(f"ğŸ“ ìˆ˜ì •ëœ íŒŒì¼ ì €ì¥ ìœ„ì¹˜: {recompressor.output_dir}")
        else:
            print("ğŸ“ ìˆ˜ì •ëœ í•­ëª©ì´ ì—†ì–´ ì¬ì••ì¶•ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        
        return
    
    # ìë™ ìˆ˜ì •ë§Œ ì‹¤í–‰
    if args.auto_fix:
        processor = ZipProcessor()
        json_files = processor.process_directory(target_path)
        
        if not json_files:
            print("âŒ JSON íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            sys.exit(1)
        
        print(f"ğŸ”§ ìë™ ìˆ˜ì • ì‹œì‘: {len(json_files)}ê°œ íŒŒì¼")
        total_fixes = 0
        
        for json_file in json_files:
            if "visualinfo" in json_file.name:
                extract_dir = json_file.parent.parent
                fixer = RuleBasedFixer(extract_dir)
                fixes = fixer.run_all_rule_fixes()
                
                fix_count = sum(len(fix_list) for fix_list in fixes.values())
                if fix_count > 0:
                    fixer.save_fixes()
                    total_fixes += fix_count
                    print(f"  âœ… {json_file.parent.parent.name}: {fix_count}ê°œ ìˆ˜ì •")
                else:
                    print(f"  ğŸ“ {json_file.parent.parent.name}: ìˆ˜ì •í•  í•­ëª© ì—†ìŒ")
        
        print(f"\nğŸ“Š ì´ ìˆ˜ì • í•­ëª©: {total_fixes}ê°œ")
        return
    
    # ì¬ì••ì¶•ë§Œ ì‹¤í–‰
    if args.recompress:
        print("ğŸ“¦ ì¬ì••ì¶• ì‹œì‘...")
        recompressor = ZipRecompressor()
        recompressed_files = recompressor.recompress_all(Path("extracted_data"))
        
        print(f"âœ… ì¬ì••ì¶• ì™„ë£Œ!")
        print(f"ğŸ“ ì••ì¶• íŒŒì¼ ì €ì¥ ìœ„ì¹˜: {recompressor.output_dir}")
        return
    
    start_time = time.time()
    
    try:
        if target_path.is_file():
            print(f"ğŸ“„ íŒŒì¼ ê²€ìˆ˜: {target_path}")
            issues = controller.validate_file(target_path)
            
            if args.fix and issues:
                print("ğŸ”§ ìë™ ìˆ˜ì • ì‹¤í–‰ ì¤‘...")
                fixed_issues = controller.auto_fix_file(target_path)
                print(f"âœ… ìˆ˜ì • ì™„ë£Œ: {len(issues) - len(fixed_issues)}ê°œ ì´ìŠˆ í•´ê²°")
            
        else:
            print(f"ğŸ“ ë””ë ‰í† ë¦¬ ê²€ìˆ˜: {target_path}")
            all_issues = controller.validate_directory(target_path)
            
            if args.fix and all_issues:
                print("ğŸ”§ ì¼ê´„ ìë™ ìˆ˜ì • ì‹¤í–‰ ì¤‘...")
                for file_path in all_issues.keys():
                    controller.auto_fix_file(Path(file_path))
                print("âœ… ì¼ê´„ ìˆ˜ì • ì™„ë£Œ")
            
            # ë³´ê³ ì„œ ìƒì„±
            processing_time = time.time() - start_time
            report = controller.generate_report(all_issues, processing_time)
            
            print(f"\nğŸ“Š ê²€ìˆ˜ ê²°ê³¼:")
            print(f"  ì´ íŒŒì¼: {report.total_files}ê°œ")
            print(f"  ì´ ì´ìŠˆ: {report.total_issues}ê°œ")
            print(f"  ì²˜ë¦¬ ì‹œê°„: {report.processing_time:.2f}ì´ˆ")
            
            if args.report:
                controller.export_report(report, Path(args.report))
                print(f"ğŸ“„ ë³´ê³ ì„œ ì €ì¥: {args.report}")
        
    except KeyboardInterrupt:
        print("\nâš ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        sys.exit(1)
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()
