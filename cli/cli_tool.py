#!/usr/bin/env python3
"""
CLI ë„êµ¬ - í’ˆì§ˆ ê²€ìˆ˜ ëª…ë ¹í–‰ ì¸í„°í˜ì´ìŠ¤
"""

import argparse
import sys
import time
import json
import re
from pathlib import Path
import PyPDF2

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ sys.pathì— ì¶”ê°€
sys.path.insert(0, str(Path(__file__).parent.parent))

from src.core.quality_controller import QualityController
from src.utils.quality_comparator import QualityComparator
from src.utils.zip_processor import ZipProcessor
from src.utils.zip_recompressor import ZipRecompressor
from src.core.rule_fixer import RuleBasedFixer


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(
        description="""
ğŸ“‹ ë¼ë²¨ë§ í’ˆì§ˆ ê²€ìˆ˜ ë„êµ¬ v2.0

ì´ ë„êµ¬ëŠ” ë¼ë²¨ë§ ë°ì´í„°ì˜ í’ˆì§ˆì„ ê²€ì¦í•˜ê³  ìë™ìœ¼ë¡œ ìˆ˜ì •í•˜ëŠ” CLI ë„êµ¬ì…ë‹ˆë‹¤.
JSON íŒŒì¼ ë‚´ì˜ ë¼ë²¨ë§ ì˜¤ë¥˜ë¥¼ ê°ì§€í•˜ê³ , í•œê¸€ ì¸ì½”ë”© ë¬¸ì œê¹Œì§€ í¬í•¨í•˜ì—¬ ì¢…í•©ì ì¸ í’ˆì§ˆ ê´€ë¦¬ë¥¼ ì œê³µí•©ë‹ˆë‹¤.

ğŸ” ì§€ì›ë˜ëŠ” ê²€ì¦ ê·œì¹™:
  R001: ì›ë¬¸/ë²ˆì—­ë¬¸ì´ ListTextì¸ ê²½ìš° â†’ ParaTextë¡œ ë³€ê²½
  R002: ìˆ«ìë‚˜ â–¡ê°€ ì—†ëŠ” RegionTitle â†’ ParaTitleë¡œ ë³€ê²½  
  R003: ë²•ë ¹ êµ¬ì¡° ë¼ë²¨ë§ (ì œ~í¸/ì¥/ì ˆ/ì¡°, ~ë²• â†’ ParaTitle)
  R004: í•œê¸€ ì¸ì½”ë”© ì˜¤ë¥˜ ê²€ì¶œ (ê¹¨ì§„ í…ìŠ¤íŠ¸, ì˜ëª»ëœ ì˜ì–´ ì¸ì‹)

ğŸ’¡ ì‚¬ìš© ì˜ˆì‹œ:
  # ë‹¨ìˆœ ê²€ì¦ë§Œ ì‹¤í–‰
  python cli/cli_tool.py "data/" --validate
  
  # ìë™ ìˆ˜ì • ì‹¤í–‰
  python cli/cli_tool.py "data/" --auto-fix
  
  # ê²€ìˆ˜ í´ë” ìƒì„±í•˜ì—¬ ì „ì²´ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰
  python cli/cli_tool.py "ì›ë³¸í´ë”/" --process-to-review
  
  # ëª¨ë“  ë¼ë²¨ì„ ListTextë¡œ í†µì¼ í›„ R003 ë²•ë ¹ êµ¬ì¡°ë§Œ ì ìš©
  python cli/cli_tool.py "zipí´ë”/" --listtext-only
  
  # ë‘ í´ë” ë¹„êµ ë¶„ì„
  python cli/cli_tool.py "ì›ë³¸/" --compare "ê²€ìˆ˜ì™„ë£Œ/"
  
  # ì „ì²´ ì›Œí¬í”Œë¡œìš° (ì¶”ì¶œâ†’ìˆ˜ì •â†’ì¬ì••ì¶•)
  python cli/cli_tool.py "data/" --full-workflow
        """,
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ğŸ“š ìƒì„¸ ì˜µì…˜ ì„¤ëª…:

ğŸ’¼ ê¸°ë³¸ ì‘ì—… ëª¨ë“œ:
  --validate (-v)     : í’ˆì§ˆ ê²€ì¦ë§Œ ì‹¤í–‰í•˜ê³  ê²°ê³¼ ì¶œë ¥
  --auto-fix          : ê²€ì¦ í›„ ìë™ ìˆ˜ì •ê¹Œì§€ ì‹¤í–‰
  --process-to-review : ê²€ìˆ˜ í´ë” ìƒì„±í•˜ì—¬ ìˆ˜ì •ëœ íŒŒì¼ ì €ì¥ (ê¶Œì¥)

ğŸ”§ ê³ ê¸‰ ì˜µì…˜:
  --full-workflow     : ZIP ì¶”ì¶œ â†’ ìˆ˜ì • â†’ ì¬ì••ì¶• ì „ì²´ ê³¼ì •
  --recompress        : ìˆ˜ì • í›„ ZIP íŒŒì¼ë¡œ ì¬ì••ì¶•
  --compare (-c)      : ë‘ í´ë”ì˜ ê²€ìˆ˜ ê²°ê³¼ ë¹„êµ

ğŸ“Š ì¶œë ¥ ì˜µì…˜:
  --report (-r)       : ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥
  --verbose           : ìƒì„¸í•œ ì²˜ë¦¬ ê³¼ì • ì¶œë ¥

ğŸš€ ê¶Œì¥ ì‚¬ìš©ë²•:
  1. í’ˆì§ˆ í™•ì¸: python cli/cli_tool.py "í´ë”ê²½ë¡œ/" --validate
  2. ìë™ ìˆ˜ì •: python cli/cli_tool.py "í´ë”ê²½ë¡œ/" --process-to-review
  3. ê²°ê³¼ ë¹„êµ: python cli/cli_tool.py "ì›ë³¸/" --compare "ìˆ˜ì •ë³¸/"

âš ï¸  ì£¼ì˜ì‚¬í•­:
  - ì›ë³¸ íŒŒì¼ ë°±ì—…ì„ ê¶Œì¥í•©ë‹ˆë‹¤
  - --process-to-review ì˜µì…˜ì´ ê°€ì¥ ì•ˆì „í•œ ë°©ë²•ì…ë‹ˆë‹¤
  - R004 ê·œì¹™ì€ í•œê¸€ ê¹¨ì§ì„ ê°ì§€í•˜ì§€ë§Œ ìë™ ìˆ˜ì •í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤
        """
    )
    
    # í•„ìˆ˜ ì¸ìˆ˜
    parser.add_argument(
        "path", 
        help="ê²€ìˆ˜í•  íŒŒì¼ ë˜ëŠ” ë””ë ‰í† ë¦¬ ê²½ë¡œ (ZIP íŒŒì¼ í¬í•¨ í´ë” ë˜ëŠ” JSON íŒŒì¼)"
    )
    
    # ì‘ì—… ëª¨ë“œ ì˜µì…˜
    mode_group = parser.add_argument_group('ì‘ì—… ëª¨ë“œ', 'ê²€ìˆ˜ ì‘ì—…ì˜ ì¢…ë¥˜ë¥¼ ì„ íƒí•˜ì„¸ìš”')
    mode_group.add_argument(
        "--validate", "-v", 
        action="store_true", 
        help="í’ˆì§ˆ ê²€ì¦ë§Œ ì‹¤í–‰ (ìˆ˜ì •í•˜ì§€ ì•ŠìŒ)"
    )
    mode_group.add_argument(
        "--auto-fix", 
        action="store_true", 
        help="ìë™ ìˆ˜ì • ì‹¤í–‰ (ì›ë³¸ íŒŒì¼ ì§ì ‘ ìˆ˜ì •)"
    )
    mode_group.add_argument(
        "--process-to-review", 
        action="store_true", 
        help="ê²€ìˆ˜ í´ë” ìƒì„±í•˜ì—¬ ìˆ˜ì •ëœ íŒŒì¼ ì €ì¥ (ê¶Œì¥ ë°©ë²•)"
    )
    mode_group.add_argument(
        "--full-workflow", 
        action="store_true", 
        help="ì „ì²´ ì›Œí¬í”Œë¡œìš°: ZIP ì¶”ì¶œ â†’ í’ˆì§ˆê²€ìˆ˜ â†’ ìë™ìˆ˜ì • â†’ ì¬ì••ì¶•"
    )
    mode_group.add_argument(
        "--listtext-only", 
        action="store_true", 
        help="ëª¨ë“  ë¼ë²¨(ParaText, RegionTitle ë“±)ì„ ListTextë¡œ í†µì¼ í›„ R003 ë²•ë ¹ êµ¬ì¡° ê·œì¹™ë§Œ ì ìš©"
    )

    mode_group.add_argument(
        "--listtext-only2", 
        action="store_true", 
        help="ëª¨ë“  ë¼ë²¨(ParaText, RegionTitle ë“±)ì„ ListTextë¡œ í†µì¼ í›„ R003 ë²•ë ¹ êµ¬ì¡° ê·œì¹™ë§Œ ì ìš©"
    )
    mode_group.add_argument(
        "--count-pages", 
        action="store_true", 
        help="ZIP íŒŒì¼ë“¤ì˜ ì´ í˜ì´ì§€ ìˆ˜ë¥¼ ë¶„ì„í•˜ê³  í†µê³„ ì •ë³´ ì¶œë ¥"
    )
    
    # ë¹„êµ ë° ë¶„ì„ ì˜µì…˜
    analysis_group = parser.add_argument_group('ë¹„êµ ë° ë¶„ì„', 'ê²€ìˆ˜ ê²°ê³¼ë¥¼ ë¹„êµí•˜ê³  ë¶„ì„í•©ë‹ˆë‹¤')
    analysis_group.add_argument(
        "--compare", "-c", 
        metavar="ì™„ë£Œí´ë”ê²½ë¡œ",
        help="ê²€ìˆ˜ ì™„ë£Œ í´ë”ì™€ ë¹„êµí•˜ì—¬ ì •í™•ë„ ë¶„ì„"
    )
    
    # ì¶œë ¥ ë° ë³´ê³ ì„œ ì˜µì…˜
    output_group = parser.add_argument_group('ì¶œë ¥ ë° ë³´ê³ ì„œ', 'ê²°ê³¼ ì €ì¥ ë° ì¶œë ¥ ë°©ì‹ì„ ì„¤ì •í•©ë‹ˆë‹¤')
    output_group.add_argument(
        "--report", "-r", 
        metavar="ë³´ê³ ì„œíŒŒì¼.json",
        help="ê²€ìˆ˜ ê²°ê³¼ë¥¼ JSON ë³´ê³ ì„œë¡œ ì €ì¥í•  ê²½ë¡œ"
    )
    output_group.add_argument(
        "--verbose", 
        action="store_true", 
        help="ìƒì„¸í•œ ì²˜ë¦¬ ê³¼ì •ê³¼ ë””ë²„ê·¸ ì •ë³´ ì¶œë ¥"
    )
    
    # ê³ ê¸‰ ì˜µì…˜ (í•˜ìœ„ í˜¸í™˜ì„±)
    advanced_group = parser.add_argument_group('ê³ ê¸‰ ì˜µì…˜', 'íŠ¹ìˆ˜í•œ ìš©ë„ë¡œ ì‚¬ìš©ë˜ëŠ” ì˜µì…˜ë“¤')
    advanced_group.add_argument(
        "--fix", "-f", 
        action="store_true", 
        help="ìë™ ìˆ˜ì • ì‹¤í–‰ (--auto-fixì™€ ë™ì¼, í•˜ìœ„ í˜¸í™˜ì„±ìš©)"
    )
    advanced_group.add_argument(
        "--recompress", 
        action="store_true", 
        help="ìˆ˜ì • í›„ ZIP íŒŒì¼ë¡œ ì¬ì••ì¶•"
    )
    
    args = parser.parse_args()
    
    # ê²½ë¡œ ê²€ì¦
    target_path = Path(args.path)
    if not target_path.exists():
        print(f"âŒ ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {target_path}")
        sys.exit(1)
    
    # ì»¨íŠ¸ë¡¤ëŸ¬ ì´ˆê¸°í™”
    controller = QualityController()
    
    # ë¹„êµ ëª¨ë“œ
    if args.compare:
        comparator = QualityComparator()
        completed_dir = Path(args.compare)
        
        if not completed_dir.exists():
            print(f"âŒ ê²€ìˆ˜ì™„ë£Œ í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {completed_dir}")
            sys.exit(1)
        
        print(f"ğŸ” ê²€ìˆ˜ ë¹„êµ ì‹œì‘")
        print(f"ğŸ“‚ ê²€ìˆ˜ ëŒ€ìƒ: {target_path}")
        print(f"ğŸ“‚ ê²€ìˆ˜ ì™„ë£Œ: {completed_dir}")
        
        results = comparator.compare_directories(target_path, completed_dir)
        report = comparator.generate_comparison_report(results)
        comparator.print_summary(report)
        
        if args.report:
            with open(args.report, 'w', encoding='utf-8') as f:
                json.dump(report, f, ensure_ascii=False, indent=2)
            print(f"ğŸ“„ ë¹„êµ ë³´ê³ ì„œ ì €ì¥: {args.report}")
        
        return
    
    # í˜ì´ì§€ ìˆ˜ ê³„ì‚° ëª¨ë“œ
    if args.count_pages:
        print("ğŸ“Š í˜ì´ì§€ ìˆ˜ ë¶„ì„ ì‹œì‘")
        
        # ì„ì‹œ ì¶”ì¶œ ë””ë ‰í† ë¦¬ ìƒì„±
        temp_extract_dir = target_path / "temp_page_count"
        temp_extract_dir.mkdir(exist_ok=True)
        
        try:
            # ZIP íŒŒì¼ ì¶”ì¶œ
            processor = ZipProcessor(extract_base_dir=temp_extract_dir)
            json_files = processor.process_directory(target_path)

            if not json_files:
                print("âŒ JSON íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                import shutil
                shutil.rmtree(temp_extract_dir, ignore_errors=True)
                sys.exit(1)

            # í˜ì´ì§€ ìˆ˜ ê³„ì‚° - PDF íŒŒì¼ ê¸°ë°˜ìœ¼ë¡œ ì •í™•íˆ ê³„ì‚°
            total_pages = 0
            file_pages = {}
            zip_files = list(target_path.glob("*.zip"))
            
            print(f"\nğŸ“‚ ë¶„ì„ ëŒ€ìƒ: {len(zip_files)}ê°œ ZIP íŒŒì¼")
            
            # ê° ZIP íŒŒì¼ì˜ ì¶”ì¶œëœ í´ë”ì—ì„œ PDF íŒŒì¼ ì°¾ê¸°
            for zip_file in zip_files:
                zip_name = zip_file.stem
                if zip_name.startswith('visualcontent-'):
                    doc_name = zip_name.replace('visualcontent-', '')
                else:
                    doc_name = zip_name
                
                # ì¶”ì¶œëœ í´ë”ì—ì„œ original í´ë”ì˜ PDF íŒŒì¼ ì°¾ê¸°
                extracted_folder = temp_extract_dir / zip_name
                original_folder = extracted_folder / "original"
                
                if original_folder.exists():
                    pdf_files = list(original_folder.glob("*.pdf"))
                    if pdf_files:
                        pdf_file = pdf_files[0]  # ì²« ë²ˆì§¸ PDF íŒŒì¼ ì‚¬ìš©
                        try:
                            # PDF íŒŒì¼ì˜ í˜ì´ì§€ ìˆ˜ ì½ê¸°
                            with open(pdf_file, 'rb') as file:
                                pdf_reader = PyPDF2.PdfReader(file)
                                pages_count = len(pdf_reader.pages)
                                
                            file_pages[doc_name] = pages_count
                            total_pages += pages_count
                            
                        except Exception as e:
                            print(f"âš ï¸ {pdf_file.name} PDF ì½ê¸° ì˜¤ë¥˜: {e}")
                            # PDF ì½ê¸° ì‹¤íŒ¨ì‹œ JSON ê¸°ë°˜ìœ¼ë¡œ ì¶”ì •
                            file_pages[doc_name] = 1
                            total_pages += 1
                    else:
                        print(f"âš ï¸ {doc_name}: original í´ë”ì— PDF íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤")
                        file_pages[doc_name] = 1
                        total_pages += 1
                else:
                    print(f"âš ï¸ {doc_name}: original í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                    file_pages[doc_name] = 1
                    total_pages += 1

            # ê²°ê³¼ ì¶œë ¥
            print(f"\nğŸ“Š í˜ì´ì§€ ìˆ˜ ë¶„ì„ ê²°ê³¼:")
            print(f"=" * 50)
            
            for file_name, pages in sorted(file_pages.items()):
                print(f"ğŸ“„ {file_name}: {pages}í˜ì´ì§€")
            
            print(f"=" * 50)
            print(f"ğŸ“š ì´ {len(file_pages)}ê°œ ë¬¸ì„œ")
            print(f"ğŸ“– ì´ í˜ì´ì§€ ìˆ˜: {total_pages}í˜ì´ì§€")
            print(f"ğŸ“Š í‰ê·  í˜ì´ì§€ ìˆ˜: {total_pages/len(file_pages):.1f}í˜ì´ì§€" if file_pages else "ğŸ“Š í‰ê· : 0í˜ì´ì§€")
            
            # ë³´ê³ ì„œ ì €ì¥
            if args.report:
                page_report = {
                    "analysis_date": time.strftime("%Y-%m-%d %H:%M:%S"),
                    "total_documents": len(file_pages),
                    "total_pages": total_pages,
                    "average_pages": round(total_pages/len(file_pages), 1) if file_pages else 0,
                    "documents": file_pages
                }
                
                with open(args.report, 'w', encoding='utf-8') as f:
                    json.dump(page_report, f, ensure_ascii=False, indent=2)
                print(f"ğŸ“„ í˜ì´ì§€ ìˆ˜ ë³´ê³ ì„œ ì €ì¥: {args.report}")
        
        finally:
            # ì„ì‹œ ë””ë ‰í† ë¦¬ ì •ë¦¬
            import shutil
            shutil.rmtree(temp_extract_dir, ignore_errors=True)
        
        return
    
    # ListText í†µì¼ + R003 ë²•ë ¹ êµ¬ì¡° ì ìš© ëª¨ë“œ
    if args.listtext_only:
        print("ğŸ“ ListText í†µì¼ + R003 ë²•ë ¹ êµ¬ì¡° ì ìš© ì‹œì‘")
        
        # ê²€ìˆ˜ í´ë” ë° ë‚´ë¶€ extracted_data í´ë” ìƒì„± (ZIP íŒŒì¼ì´ ìˆëŠ” ë””ë ‰í† ë¦¬ì—)
        review_dir = target_path / "ê²€ìˆ˜_ListText"
        review_dir.mkdir(exist_ok=True)
        extracted_dir = review_dir / "extracted_data"
        extracted_dir.mkdir(exist_ok=True)
        print(f"ğŸ“ ê²€ìˆ˜ í´ë” ìƒì„±: {review_dir}")
        print(f"ğŸ“ ì¶”ì¶œ í´ë” ìƒì„±: {extracted_dir}")

        # 1. ZIP íŒŒì¼ ì¶”ì¶œ
        processor = ZipProcessor(extract_base_dir=extracted_dir)
        json_files = processor.process_directory(target_path)

        if not json_files:
            print("âŒ JSON íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            sys.exit(1)

        # 2. ListText í†µì¼ + R003 ì ìš©
        print(f"\nğŸ”§ ListText í†µì¼ ë° R003 ë²•ë ¹ êµ¬ì¡° ì ìš©: {len(json_files)}ê°œ íŒŒì¼")
        total_changes = 0

        for json_file in json_files:
            if "visualinfo" in json_file.name:
                try:
                    # JSON íŒŒì¼ ë¡œë“œ
                    with open(json_file, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                    
                    elements = data.get('elements', [])
                    changes = 0
                    
                    for element in elements:
                        category = element.get('category', {})
                        text = element.get('content', {}).get('text', '').strip()
                        current_label = category.get('label', '')
                        
                        # 1ë‹¨ê³„: PageNumberë¥¼ ì œì™¸í•˜ê³  ëª¨ë“  ê²ƒì„ ListTextë¡œ ë³€ê²½
                        if current_label != 'ListText' and current_label != 'PageNumber':
                            category['label'] = 'ListText'
                            category['type'] = 'LIST'  # typeë„ í•¨ê»˜ ë³€ê²½
                            changes += 1
                    
                    # 2ë‹¨ê³„: R003 ë²•ë ¹ êµ¬ì¡° ê·œì¹™ ì ìš© (ListText â†’ ParaTitle)
                    for element in elements:
                        category = element.get('category', {})
                        text = element.get('content', {}).get('text', '').strip()
                        current_label = category.get('label', '')
                        
                        if current_label == 'ListText' and text:
                            # ~ë²• íŒ¨í„´
                            if re.search(r'[ê°€-í£]+ë²•$', text):
                                category['label'] = 'ParaTitle'
                                category['type'] = 'HEADING'  # typeë„ í•¨ê»˜ ë³€ê²½
                                changes += 1
                            # ì œ~í¸, ì œ~ì¥, ì œ~ì ˆ, ì œ~ì¡° íŒ¨í„´
                            elif re.search(r'^ì œ\s*\d+\s*(í¸|ì¥|ì ˆ|ê´€|ì¡°)', text):
                                category['label'] = 'ParaTitle'
                                category['type'] = 'HEADING'  # typeë„ í•¨ê»˜ ë³€ê²½
                                changes += 1
                    
                    if changes > 0:
                        # JSON íŒŒì¼ ì €ì¥
                        with open(json_file, 'w', encoding='utf-8') as f:
                            json.dump(data, f, ensure_ascii=False, indent=2)
                        
                        total_changes += changes
                        print(f"  âœ… {json_file.parent.parent.name}: {changes}ê°œ ë³€ê²½")
                    else:
                        print(f"  ğŸ“ {json_file.parent.parent.name}: ë³€ê²½í•  í•­ëª© ì—†ìŒ")
                        
                except Exception as e:
                    print(f"  âŒ {json_file.parent.parent.name}: ì˜¤ë¥˜ - {str(e)}")

        print(f"\nğŸ“Š ì´ ë³€ê²½ í•­ëª©: {total_changes}ê°œ")

        # 3. ì¬ì••ì¶•
        if total_changes > 0:
            print(f"\nğŸ“¦ ì¬ì••ì¶• ì¤‘...")
            recompressor = ZipRecompressor(review_dir)
            recompressed_files = recompressor.recompress_all(extracted_dir)

            print(f"âœ… ì²˜ë¦¬ ì™„ë£Œ!")
            print(f"ğŸ“ ìˆ˜ì •ëœ íŒŒì¼ ì €ì¥ ìœ„ì¹˜: {review_dir}")
            print(f"ğŸ“¦ ìƒì„±ëœ íŒŒì¼: {len(recompressed_files)}ê°œ")
        else:
            print("ğŸ“ ë³€ê²½ì‚¬í•­ì´ ì—†ì–´ ì¬ì••ì¶•í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

        return
    
    if args.listtext_only2:
        print("ğŸ“ ListText í†µì¼ + R003 ë²•ë ¹ êµ¬ì¡° ì ìš© ì‹œì‘")
        
        # ê²€ìˆ˜ í´ë” ë° ë‚´ë¶€ extracted_data í´ë” ìƒì„± (ZIP íŒŒì¼ì´ ìˆëŠ” ë””ë ‰í† ë¦¬ì—)
        review_dir = target_path / "ê²€ìˆ˜_ListText"
        review_dir.mkdir(exist_ok=True)
        extracted_dir = review_dir / "extracted_data"
        extracted_dir.mkdir(exist_ok=True)
        print(f"ğŸ“ ê²€ìˆ˜ í´ë” ìƒì„±: {review_dir}")
        print(f"ğŸ“ ì¶”ì¶œ í´ë” ìƒì„±: {extracted_dir}")

        # 1. ZIP íŒŒì¼ ì¶”ì¶œ
        processor = ZipProcessor(extract_base_dir=extracted_dir)
        json_files = processor.process_directory(target_path)

        if not json_files:
            print("âŒ JSON íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            sys.exit(1)

        # 2. ListText í†µì¼ + R003 ì ìš©
        print(f"\nğŸ”§ ListText í†µì¼ ë° R003 ë²•ë ¹ êµ¬ì¡° ì ìš©: {len(json_files)}ê°œ íŒŒì¼")
        total_changes = 0

        for json_file in json_files:
            if "visualinfo" in json_file.name:
                try:
                    # JSON íŒŒì¼ ë¡œë“œ
                    with open(json_file, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                    
                    elements = data.get('elements', [])
                    changes = 0
                    
                    for element in elements:
                        category = element.get('category', {})
                        text = element.get('content', {}).get('text', '').strip()
                        current_label = category.get('label', '')
                        
                        # 1ë‹¨ê³„: PageNumberë¥¼ ì œì™¸í•˜ê³  ëª¨ë“  ê²ƒì„ ListTextë¡œ ë³€ê²½
                        if current_label != 'ListText' and current_label != 'PageNumber':
                            category['label'] = 'ListText'
                            category['type'] = 'LIST'  # typeë„ í•¨ê»˜ ë³€ê²½
                            changes += 1
                    
                    # 2ë‹¨ê³„: R003 ë²•ë ¹ êµ¬ì¡° ê·œì¹™ ì ìš© (ListText â†’ ParaTitle)
                    for element in elements:
                        category = element.get('category', {})
                        text = element.get('content', {}).get('text', '').strip()
                        current_label = category.get('label', '')
                        
                        if current_label == 'ListText' and text:
                            # ~ë²• íŒ¨í„´
                            if re.search(r'[ê°€-í£]+ë²•$', text):
                                category['label'] = 'ParaTitle'
                                category['type'] = 'HEADING'  # typeë„ í•¨ê»˜ ë³€ê²½
                                changes += 1
                            # ì œ~í¸, ì œ~ì¥, ì œ~ì ˆ, ì œ~ì¡° íŒ¨í„´
                            elif re.search(r'^ì œ\s*\d+\s*(í¸|ì¥|ì ˆ|ê´€|ì¡°)', text):
                                category['label'] = 'ParaTitle'
                                category['type'] = 'HEADING'  # typeë„ í•¨ê»˜ ë³€ê²½
                                changes += 1
                    
                    if changes > 0:
                        # JSON íŒŒì¼ ì €ì¥
                        with open(json_file, 'w', encoding='utf-8') as f:
                            json.dump(data, f, ensure_ascii=False, indent=2)
                        
                        total_changes += changes
                        print(f"  âœ… {json_file.parent.parent.name}: {changes}ê°œ ë³€ê²½")
                    else:
                        print(f"  ğŸ“ {json_file.parent.parent.name}: ë³€ê²½í•  í•­ëª© ì—†ìŒ")
                        
                except Exception as e:
                    print(f"  âŒ {json_file.parent.parent.name}: ì˜¤ë¥˜ - {str(e)}")

        print(f"\nğŸ“Š ì´ ë³€ê²½ í•­ëª©: {total_changes}ê°œ")

        # 3. ì¬ì••ì¶•
        if total_changes > 0:
            print(f"\nğŸ“¦ ì¬ì••ì¶• ì¤‘...")
            recompressor = ZipRecompressor(review_dir)
            recompressed_files = recompressor.recompress_all(extracted_dir)

            print(f"âœ… ì²˜ë¦¬ ì™„ë£Œ!")
            print(f"ğŸ“ ìˆ˜ì •ëœ íŒŒì¼ ì €ì¥ ìœ„ì¹˜: {review_dir}")
            print(f"ğŸ“¦ ìƒì„±ëœ íŒŒì¼: {len(recompressed_files)}ê°œ")
        else:
            print("ğŸ“ ë³€ê²½ì‚¬í•­ì´ ì—†ì–´ ì¬ì••ì¶•í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

        return
    
    # ì§€ì • í´ë”ì— ê²€ìˆ˜ í´ë” ìƒì„±í•˜ì—¬ ì²˜ë¦¬
    if args.process_to_review:
        print("ğŸš€ ê²€ìˆ˜ í´ë” ìƒì„± ë° ìë™ ìˆ˜ì • ì‹œì‘")
        
        # ê²€ìˆ˜ í´ë” ë° ë‚´ë¶€ extracted_data í´ë” ìƒì„±
        review_dir = target_path / "ê²€ìˆ˜"
        review_dir.mkdir(exist_ok=True)
        extracted_dir = review_dir / "extracted_data"
        extracted_dir.mkdir(exist_ok=True)
        print(f"ğŸ“ ê²€ìˆ˜ í´ë” ìƒì„±: {review_dir}")
        print(f"ğŸ“ ì¶”ì¶œ í´ë” ìƒì„±: {extracted_dir}")

        # 1. ZIP íŒŒì¼ ì¶”ì¶œ (ê²€ìˆ˜í´ë” ë‚´ë¶€ extracted_dataì— ì¶”ì¶œ)
        processor = ZipProcessor(extract_base_dir=extracted_dir)
        json_files = processor.process_directory(target_path)

        if not json_files:
            print("âŒ JSON íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            sys.exit(1)

        # 2. ìë™ ìˆ˜ì • (extracted_dir ë‚´ë¶€ íŒŒì¼ë§Œ ëŒ€ìƒìœ¼ë¡œ)
        print(f"\nğŸ”§ ìë™ ìˆ˜ì • ì‹œì‘: {len(json_files)}ê°œ íŒŒì¼")
        total_fixes = 0

        for json_file in json_files:
            if "visualinfo" in json_file.name:
                extract_dir = json_file.parent.parent
                fixer = RuleBasedFixer(extract_dir)
                fixes = fixer.run_all_rule_fixes()

                fix_count = sum(len(fix_list) for fix_list in fixes.values())
                if fix_count > 0:
                    fixer.save_fixes()
                    total_fixes += fix_count
                    print(f"  âœ… {json_file.parent.parent.name}: {fix_count}ê°œ ìˆ˜ì •")
                else:
                    print(f"  ğŸ“ {json_file.parent.parent.name}: ìˆ˜ì •í•  í•­ëª© ì—†ìŒ")

        print(f"\nğŸ“Š ì´ ìˆ˜ì • í•­ëª©: {total_fixes}ê°œ")

        # 3. ê²€ìˆ˜ í´ë”ì— ì¬ì••ì¶• (extracted_dirì—ì„œ review_dirë¡œ)
        if total_fixes > 0:
            print(f"\nğŸ“¦ ê²€ìˆ˜ í´ë”ì— ì¬ì••ì¶• ì¤‘...")
            recompressor = ZipRecompressor(review_dir)
            recompressed_files = recompressor.recompress_all(extracted_dir)

            print(f"âœ… ì²˜ë¦¬ ì™„ë£Œ!")
            print(f"ğŸ“ ìˆ˜ì •ëœ íŒŒì¼ ì €ì¥ ìœ„ì¹˜: {review_dir}")
            print(f"ğŸ“¦ ìƒì„±ëœ íŒŒì¼: {len(recompressed_files)}ê°œ")

            # íŒŒì¼ëª…ì—ì„œ _fixed ì œê±°í•˜ì—¬ ì›ë³¸ ì´ë¦„ìœ¼ë¡œ ë³€ê²½
            for zip_file in recompressed_files:
                original_name = zip_file.name.replace("_fixed.zip", ".zip")
                new_path = zip_file.parent / original_name

                # ê¸°ì¡´ íŒŒì¼ì´ ìˆìœ¼ë©´ ì‚­ì œ
                if new_path.exists():
                    new_path.unlink()

                zip_file.rename(new_path)
                print(f"  ğŸ“„ {original_name}")
        else:
            print("ğŸ“ ìˆ˜ì •ëœ í•­ëª©ì´ ì—†ì–´ ì¬ì••ì¶•ì„ ê±´ë„ˆëœë‹ˆë‹¤.")

        return
    
    # ì „ì²´ ì›Œí¬í”Œë¡œìš° ëª¨ë“œ (ì¶”ì¶œ â†’ ìë™ìˆ˜ì • â†’ ì¬ì••ì¶•)
    if args.full_workflow:
        print("ğŸš€ ì „ì²´ ì›Œí¬í”Œë¡œìš° ì‹œì‘: ì¶”ì¶œ â†’ ìë™ìˆ˜ì • â†’ ì¬ì••ì¶•")
        
        # 1. ZIP íŒŒì¼ ì¶”ì¶œ
        processor = ZipProcessor()
        json_files = processor.process_directory(target_path)
        
        if not json_files:
            print("âŒ JSON íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            sys.exit(1)
        
        # 2. ìë™ ìˆ˜ì •
        print(f"\nğŸ”§ ìë™ ìˆ˜ì • ì‹œì‘: {len(json_files)}ê°œ íŒŒì¼")
        total_fixes = 0
        
        for json_file in json_files:
            if "visualinfo" in json_file.name:
                extract_dir = json_file.parent.parent
                fixer = RuleBasedFixer(extract_dir)
                fixes = fixer.run_all_rule_fixes()
                
                fix_count = sum(len(fix_list) for fix_list in fixes.values())
                if fix_count > 0:
                    fixer.save_fixes()
                    total_fixes += fix_count
                    print(f"  âœ… {json_file.parent.parent.name}: {fix_count}ê°œ ìˆ˜ì •")
                else:
                    print(f"  ğŸ“ {json_file.parent.parent.name}: ìˆ˜ì •í•  í•­ëª© ì—†ìŒ")
        
        print(f"\nğŸ“Š ì´ ìˆ˜ì • í•­ëª©: {total_fixes}ê°œ")
        
        # 3. ì¬ì••ì¶•
        if total_fixes > 0:
            print(f"\nğŸ“¦ ì¬ì••ì¶• ì‹œì‘...")
            recompressor = ZipRecompressor()
            recompressed_files = recompressor.recompress_all(Path("extracted_data"))
            
            print(f"âœ… ì›Œí¬í”Œë¡œìš° ì™„ë£Œ!")
            print(f"ğŸ“ ìˆ˜ì •ëœ íŒŒì¼ ì €ì¥ ìœ„ì¹˜: {recompressor.output_dir}")
        else:
            print("ğŸ“ ìˆ˜ì •ëœ í•­ëª©ì´ ì—†ì–´ ì¬ì••ì¶•ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        
        return
    
    # ìë™ ìˆ˜ì •ë§Œ ì‹¤í–‰
    if args.auto_fix:
        processor = ZipProcessor()
        json_files = processor.process_directory(target_path)
        
        if not json_files:
            print("âŒ JSON íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            sys.exit(1)
        
        print(f"ğŸ”§ ìë™ ìˆ˜ì • ì‹œì‘: {len(json_files)}ê°œ íŒŒì¼")
        total_fixes = 0
        
        for json_file in json_files:
            if "visualinfo" in json_file.name:
                extract_dir = json_file.parent.parent
                fixer = RuleBasedFixer(extract_dir)
                fixes = fixer.run_all_rule_fixes()
                
                fix_count = sum(len(fix_list) for fix_list in fixes.values())
                if fix_count > 0:
                    fixer.save_fixes()
                    total_fixes += fix_count
                    print(f"  âœ… {json_file.parent.parent.name}: {fix_count}ê°œ ìˆ˜ì •")
                else:
                    print(f"  ğŸ“ {json_file.parent.parent.name}: ìˆ˜ì •í•  í•­ëª© ì—†ìŒ")
        
        print(f"\nğŸ“Š ì´ ìˆ˜ì • í•­ëª©: {total_fixes}ê°œ")
        return
    
    # ì¬ì••ì¶•ë§Œ ì‹¤í–‰
    if args.recompress:
        print("ğŸ“¦ ì¬ì••ì¶• ì‹œì‘...")
        recompressor = ZipRecompressor()
        recompressed_files = recompressor.recompress_all(Path("extracted_data"))
        
        print(f"âœ… ì¬ì••ì¶• ì™„ë£Œ!")
        print(f"ğŸ“ ì••ì¶• íŒŒì¼ ì €ì¥ ìœ„ì¹˜: {recompressor.output_dir}")
        return
    
    start_time = time.time()
    
    try:
        if target_path.is_file():
            print(f"ğŸ“„ íŒŒì¼ ê²€ìˆ˜: {target_path}")
            issues = controller.validate_file(target_path)
            
            if args.fix and issues:
                print("ğŸ”§ ìë™ ìˆ˜ì • ì‹¤í–‰ ì¤‘...")
                fixed_issues = controller.auto_fix_file(target_path)
                print(f"âœ… ìˆ˜ì • ì™„ë£Œ: {len(issues) - len(fixed_issues)}ê°œ ì´ìŠˆ í•´ê²°")
            
        else:
            print(f"ğŸ“ ë””ë ‰í† ë¦¬ ê²€ìˆ˜: {target_path}")
            all_issues = controller.validate_directory(target_path)
            
            if args.fix and all_issues:
                print("ğŸ”§ ì¼ê´„ ìë™ ìˆ˜ì • ì‹¤í–‰ ì¤‘...")
                for file_path in all_issues.keys():
                    controller.auto_fix_file(Path(file_path))
                print("âœ… ì¼ê´„ ìˆ˜ì • ì™„ë£Œ")
            
            # ë³´ê³ ì„œ ìƒì„±
            processing_time = time.time() - start_time
            report = controller.generate_report(all_issues, processing_time)
            
            print(f"\nğŸ“Š ê²€ìˆ˜ ê²°ê³¼:")
            print(f"  ì´ íŒŒì¼: {report.total_files}ê°œ")
            print(f"  ì´ ì´ìŠˆ: {report.total_issues}ê°œ")
            print(f"  ì²˜ë¦¬ ì‹œê°„: {report.processing_time:.2f}ì´ˆ")
            
            if args.report:
                controller.export_report(report, Path(args.report))
                print(f"ğŸ“„ ë³´ê³ ì„œ ì €ì¥: {args.report}")
        
    except KeyboardInterrupt:
        print("\nâš ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        sys.exit(1)
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()
