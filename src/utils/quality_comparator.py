#!/usr/bin/env python3
"""
ê²€ìˆ˜ ë¹„êµ ë„êµ¬ - ìžë™ ê²€ìˆ˜ ê²°ê³¼ì™€ ìˆ˜ë™ ê²€ìˆ˜ ê²°ê³¼ë¥¼ ë¹„êµ
"""

import json
import sys
from pathlib import Path
from typing import Dict, List, Any, Tuple
from dataclasses import dataclass

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ sys.pathì— ì¶”ê°€
sys.path.insert(0, str(Path(__file__).parent.parent))

from src.core.quality_controller import QualityController
from src.models.quality_issue import QualityIssue
from src.utils.zip_processor import ZipProcessor


@dataclass
class ComparisonResult:
    """ë¹„êµ ê²°ê³¼ ë°ì´í„° í´ëž˜ìŠ¤"""
    file_path: str
    auto_issues: List[QualityIssue]
    manual_fixed: bool
    differences: List[str]
    accuracy_score: float


class QualityComparator:
    """í’ˆì§ˆ ê²€ìˆ˜ ë¹„êµê¸°"""
    
    def __init__(self):
        self.controller = QualityController()
        self.zip_processor = ZipProcessor()
    
    def compare_directories(self, target_dir: Path, completed_dir: Path) -> List[ComparisonResult]:
        """ë””ë ‰í† ë¦¬ ë¹„êµ"""
        results = []
        
        # ZIP íŒŒì¼ì´ ìžˆëŠ” ê²½ìš° ìžë™ ì¶”ì¶œ
        target_json_files = self._get_json_files(target_dir, "target")
        completed_json_files = self._get_json_files(completed_dir, "completed")
        
        print(f"ðŸ” ê²€ìˆ˜ ëŒ€ìƒ íŒŒì¼ {len(target_json_files)}ê°œ ë°œê²¬")
        print(f"ðŸ” ê²€ìˆ˜ ì™„ë£Œ íŒŒì¼ {len(completed_json_files)}ê°œ ë°œê²¬")
        
        # íŒŒì¼ëª…ìœ¼ë¡œ ë§¤ì¹­
        target_file_dict = {self._get_file_key(f): f for f in target_json_files}
        completed_file_dict = {self._get_file_key(f): f for f in completed_json_files}
        
        for file_key, target_file in target_file_dict.items():
            if file_key in completed_file_dict:
                completed_file = completed_file_dict[file_key]
                result = self._compare_single_file(target_file, completed_file)
                results.append(result)
                print(f"âœ… ë¹„êµ ì™„ë£Œ: {file_key}")
            else:
                print(f"âš ï¸ ê²€ìˆ˜ì™„ë£Œ íŒŒì¼ ì—†ìŒ: {file_key}")
        
        return results
    
    def _get_json_files(self, directory: Path, label: str) -> List[Path]:
        """ë””ë ‰í† ë¦¬ì—ì„œ JSON íŒŒì¼ ì¶”ì¶œ"""
        json_files = []
        
        # ì§ì ‘ JSON íŒŒì¼ ì°¾ê¸°
        direct_json = list(directory.rglob("*.json"))
        if direct_json:
            json_files.extend(direct_json)
            print(f"ðŸ“„ {label} ë””ë ‰í† ë¦¬ì—ì„œ ì§ì ‘ JSON íŒŒì¼ {len(direct_json)}ê°œ ë°œê²¬")
        
        # ZIP íŒŒì¼ì´ ìžˆìœ¼ë©´ ì¶”ì¶œ
        zip_files = list(directory.rglob("*.zip"))
        if zip_files:
            print(f"ðŸ“¦ {label} ë””ë ‰í† ë¦¬ì—ì„œ ZIP íŒŒì¼ {len(zip_files)}ê°œ ë°œê²¬, ì¶”ì¶œ ì¤‘...")
            extracted_json = self.zip_processor.process_directory(directory)
            json_files.extend(extracted_json)
        
        return json_files
    
    def _get_file_key(self, file_path: Path) -> str:
        """íŒŒì¼ ë§¤ì¹­ìš© í‚¤ ìƒì„±"""
        # visualinfo íŒŒì¼ëª…ì—ì„œ ê³ ìœ  ì‹ë³„ìž ì¶”ì¶œ
        name = file_path.name
        if "_visualinfo" in name:
            # "FLAW2023000592_visualinfo.json" â†’ "FLAW2023000592"
            return name.split("_visualinfo")[0]
        else:
            return file_path.stem
    
    def _compare_single_file(self, target_file: Path, completed_file: Path) -> ComparisonResult:
        """ë‹¨ì¼ íŒŒì¼ ë¹„êµ"""
        # ìžë™ ê²€ìˆ˜ ì‹¤í–‰
        auto_issues = self.controller.validate_file(target_file)
        
        # ì›ë³¸ê³¼ ê²€ìˆ˜ì™„ë£Œ íŒŒì¼ ë¹„êµ
        differences = self._find_differences(target_file, completed_file)
        
        # ì •í™•ë„ ê³„ì‚°
        accuracy = self._calculate_accuracy(auto_issues, differences)
        
        return ComparisonResult(
            file_path=str(target_file.name),
            auto_issues=auto_issues,
            manual_fixed=len(differences) > 0,
            differences=differences,
            accuracy_score=accuracy
        )
    
    def _find_differences(self, target_file: Path, completed_file: Path) -> List[str]:
        """íŒŒì¼ ê°„ ì°¨ì´ì  ì°¾ê¸°"""
        differences = []
        
        try:
            with open(target_file, 'r', encoding='utf-8') as f:
                target_data = json.load(f)
            
            with open(completed_file, 'r', encoding='utf-8') as f:
                completed_data = json.load(f)
            
            # ìš”ì†Œë³„ ë¹„êµ
            target_elements = {e.get('id'): e for e in target_data.get('elements', [])}
            completed_elements = {e.get('id'): e for e in completed_data.get('elements', [])}
            
            # ë¼ë²¨ ë³€ê²½ í™•ì¸
            for element_id, target_element in target_elements.items():
                if element_id in completed_elements:
                    completed_element = completed_elements[element_id]
                    
                    target_label = target_element.get('category', {}).get('label', '')
                    completed_label = completed_element.get('category', {}).get('label', '')
                    
                    if target_label != completed_label:
                        text = target_element.get('content', {}).get('text', '')[:30]
                        differences.append(f"ë¼ë²¨ ë³€ê²½: {element_id} '{text}...' {target_label} â†’ {completed_label}")
            
            # ìš”ì†Œ ì¶”ê°€/ì œê±° í™•ì¸
            added_elements = set(completed_elements.keys()) - set(target_elements.keys())
            removed_elements = set(target_elements.keys()) - set(completed_elements.keys())
            
            for element_id in added_elements:
                differences.append(f"ìš”ì†Œ ì¶”ê°€: {element_id}")
            
            for element_id in removed_elements:
                differences.append(f"ìš”ì†Œ ì œê±°: {element_id}")
        
        except Exception as e:
            differences.append(f"íŒŒì¼ ë¹„êµ ì˜¤ë¥˜: {str(e)}")
        
        return differences
    
    def _calculate_accuracy(self, auto_issues: List[QualityIssue], manual_differences: List[str]) -> float:
        """ì •í™•ë„ ê³„ì‚°"""
        if not manual_differences:
            # ìˆ˜ë™ ìˆ˜ì •ì´ ì—†ê³  ìžë™ ê²€ìˆ˜ì—ì„œë„ ì´ìŠˆê°€ ì—†ìœ¼ë©´ 100%
            return 100.0 if not auto_issues else 90.0
        
        # ìžë™ ê²€ìˆ˜ê°€ ë°œê²¬í•œ ì´ìŠˆì™€ ìˆ˜ë™ ìˆ˜ì •ì´ ì–¼ë§ˆë‚˜ ì¼ì¹˜í•˜ëŠ”ì§€ ê³„ì‚°
        auto_label_issues = [issue for issue in auto_issues if 'label' in issue.message.lower()]
        manual_label_changes = [diff for diff in manual_differences if 'ë¼ë²¨ ë³€ê²½' in diff]
        
        if not auto_label_issues and not manual_label_changes:
            return 100.0
        
        if not auto_label_issues:
            return 50.0  # ìžë™ ê²€ìˆ˜ê°€ ë†“ì¹œ ê²½ìš°
        
        if not manual_label_changes:
            return 70.0  # ìžë™ ê²€ìˆ˜ê°€ ê³¼ê²€ì¶œí•œ ê²½ìš°
        
        # ê°„ë‹¨í•œ ì¼ì¹˜ë„ ê³„ì‚°
        match_ratio = min(len(auto_label_issues), len(manual_label_changes)) / max(len(auto_label_issues), len(manual_label_changes))
        return match_ratio * 100
    
    def generate_comparison_report(self, results: List[ComparisonResult]) -> Dict[str, Any]:
        """ë¹„êµ ë³´ê³ ì„œ ìƒì„±"""
        total_files = len(results)
        files_with_manual_fixes = sum(1 for r in results if r.manual_fixed)
        
        # ì •í™•ë„ í†µê³„
        accuracies = [r.accuracy_score for r in results]
        avg_accuracy = sum(accuracies) / len(accuracies) if accuracies else 0
        
        # ì´ìŠˆ í†µê³„
        total_auto_issues = sum(len(r.auto_issues) for r in results)
        total_manual_changes = sum(len(r.differences) for r in results)
        
        # ê°€ìž¥ ë§Žì€ ì´ìŠˆê°€ ìžˆëŠ” íŒŒì¼ë“¤
        top_issue_files = sorted(results, key=lambda r: len(r.auto_issues), reverse=True)[:5]
        
        return {
            "summary": {
                "total_files": total_files,
                "files_with_manual_fixes": files_with_manual_fixes,
                "manual_fix_rate": (files_with_manual_fixes / total_files * 100) if total_files > 0 else 0,
                "average_accuracy": avg_accuracy,
                "total_auto_issues": total_auto_issues,
                "total_manual_changes": total_manual_changes
            },
            "accuracy_distribution": {
                "high_accuracy": sum(1 for a in accuracies if a >= 80),
                "medium_accuracy": sum(1 for a in accuracies if 60 <= a < 80),
                "low_accuracy": sum(1 for a in accuracies if a < 60)
            },
            "top_issue_files": [
                {
                    "file": r.file_path,
                    "auto_issues_count": len(r.auto_issues),
                    "manual_changes_count": len(r.differences),
                    "accuracy": r.accuracy_score
                }
                for r in top_issue_files
            ],
            "detailed_results": [
                {
                    "file": r.file_path,
                    "auto_issues": [issue.to_dict() for issue in r.auto_issues],
                    "manual_differences": r.differences,
                    "accuracy_score": r.accuracy_score
                }
                for r in results
            ]
        }
    
    def print_summary(self, report: Dict[str, Any]):
        """ìš”ì•½ ì¶œë ¥"""
        summary = report["summary"]
        
        print("\n" + "="*60)
        print("ðŸ“Š ê²€ìˆ˜ ë¹„êµ ê²°ê³¼ ìš”ì•½")
        print("="*60)
        
        print(f"ðŸ“ ì´ ë¹„êµ íŒŒì¼ ìˆ˜: {summary['total_files']}ê°œ")
        print(f"ðŸ”§ ìˆ˜ë™ ìˆ˜ì • íŒŒì¼ ìˆ˜: {summary['files_with_manual_fixes']}ê°œ ({summary['manual_fix_rate']:.1f}%)")
        print(f"ðŸŽ¯ í‰ê·  ì •í™•ë„: {summary['average_accuracy']:.1f}%")
        print(f"ðŸ” ìžë™ ê²€ì¶œ ì´ìŠˆ: {summary['total_auto_issues']}ê°œ")
        print(f"âœï¸ ìˆ˜ë™ ìˆ˜ì • ì‚¬í•­: {summary['total_manual_changes']}ê°œ")
        
        print(f"\nðŸ“ˆ ì •í™•ë„ ë¶„í¬:")
        acc_dist = report["accuracy_distribution"]
        print(f"  ë†’ìŒ (80% ì´ìƒ): {acc_dist['high_accuracy']}ê°œ")
        print(f"  ë³´í†µ (60-79%): {acc_dist['medium_accuracy']}ê°œ")
        print(f"  ë‚®ìŒ (60% ë¯¸ë§Œ): {acc_dist['low_accuracy']}ê°œ")
        
        print(f"\nðŸ”¥ ì´ìŠˆê°€ ë§Žì€ ìƒìœ„ íŒŒì¼:")
        for i, file_info in enumerate(report["top_issue_files"][:3], 1):
            print(f"  {i}. {Path(file_info['file']).name}")
            print(f"     ìžë™ê²€ì¶œ: {file_info['auto_issues_count']}ê°œ, ìˆ˜ë™ìˆ˜ì •: {file_info['manual_changes_count']}ê°œ")
            print(f"     ì •í™•ë„: {file_info['accuracy']:.1f}%")


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    # ê²€ìˆ˜ ëŒ€ìƒ í´ë”
    target_dir = Path(r"C:\Users\User\Downloads\250812_ì „ì²´_ë°•ì„ í™”\1íŽ˜ì´ì§€-ê·¸ë¦¼+ë¹„êµí‘œ+í…Œì´ë¸” ì‚½ìž…")
    
    # ê²€ìˆ˜ ì™„ë£Œ í´ë”  
    completed_dir = Path(r"C:\Users\User\Documents\ê²€ìˆ˜\20250813-ë°•ì„ í™”\1íŽ˜ì´ì§€-ê·¸ë¦¼+ë¹„êµí‘œ+í…Œì´ë¸” ì‚½ìž…")
    
    if not target_dir.exists():
        print(f"âŒ ê²€ìˆ˜ ëŒ€ìƒ í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {target_dir}")
        return
    
    if not completed_dir.exists():
        print(f"âŒ ê²€ìˆ˜ ì™„ë£Œ í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {completed_dir}")
        return
    
    print("ðŸš€ ê²€ìˆ˜ ë¹„êµ ì‹œìž‘")
    print(f"ðŸ“‚ ê²€ìˆ˜ ëŒ€ìƒ: {target_dir}")
    print(f"ðŸ“‚ ê²€ìˆ˜ ì™„ë£Œ: {completed_dir}")
    
    # ë¹„êµ ì‹¤í–‰
    comparator = QualityComparator()
    results = comparator.compare_directories(target_dir, completed_dir)
    
    # ë³´ê³ ì„œ ìƒì„±
    report = comparator.generate_comparison_report(results)
    
    # ê²°ê³¼ ì¶œë ¥
    comparator.print_summary(report)
    
    # ìƒì„¸ ë³´ê³ ì„œ ì €ìž¥
    report_file = Path("quality_comparison_report.json")
    with open(report_file, 'w', encoding='utf-8') as f:
        json.dump(report, f, ensure_ascii=False, indent=2)
    
    print(f"\nðŸ’¾ ìƒì„¸ ë³´ê³ ì„œ ì €ìž¥: {report_file}")


if __name__ == "__main__":
    main()
